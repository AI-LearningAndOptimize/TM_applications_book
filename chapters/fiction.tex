
\chapter{Fiction and Literature}

\label{ch:fiction}

This chapter considers documents that are valued not just for their information content but for their artistic expression.
There are many ways to read fiction, poetry, and rhetoric, and the choice of how we read affects the type of conclusions that we are able to make.
Scholars have traditionally focused on a ``close reading'' approach, in which the goal is to identify the specific features of a passage that convey a more general meaning, or emotion, or atmosphere.
These features might include nuances of word  selection, echoes of sound through rhyme or alliteration, or prosodic features like rhythm or cadence.

\section{Topic Models in the Humanities}

While close reading is a foundational tool in the study of literature, it is necessarily limited by its scale.
We value literature because it is one of the best ways to capture the spirit of an age, and the experiences of those who lived through it. But standard close reading methods require narrow focus and thorough interpretation.
Topic models complement close reading in two ways, as a survey method and as a means for tracing and comparing large-scale patterns.

The survey method is relatively simple, linking passages that a reader may not have known about.
Close reading is the best way to analyze a short passage of text, but which short passages of text do we want to analyze?
Because no one can read---much less close read---all the available material from a culture or time period, scholars are often left trying to make large-scale arguments about the history of literature from small-scale evidence. And this small-scale exploration is not randomly selected: the same small canon is studied in detail while the vast proportion make up the ``great unread''~\citep{moretti-00}, works that are never studied.
Identifying broad themes and then mapping those themes to their realization in different contexts may reveal works or sections of works that are ``hiding in plain sight'', unknown to modern scholarship simply through obscurity.

An alternative, and less traditional, mode of analysis is often referred to as ``distant reading'' \citep{moretti-13}.   This approach uses computer-assisted methodologies. Topic modeling has emerged as a central tool in distant reading, as a way to organize our reading of large scale patterns.

Topic analysis, viewed as a way of identifying repetitions of language or discourse through multiple works, resonates with many more familiar approaches to the study of literature.
At the broadest scale, to define a genre or a literary period is to separate a corpus into sections based on some observable criterion.
We posit a ``gothic'' literature characterized by atmospheric descriptions of castles, or a ``cyberpunk'' literature characterized by conflicted relationships with information technology.
At a smaller scale, themes or tropes reappear in different contexts.
At the most detailed level, scholars identify repeated phrases, such as the descriptive epithets used in Homeric oral poetry.

Statistical topic analysis has a similar goal, but pursues it through different means.
Rather than rigid boundaries specified by date of publication or nationality, algorithms identify genre through the repeated words that form the traces of those themes.
Topics do not represent themes themselves, but rather identify the implicit statistical regularities in word use brought about by the presence of genres, themes, and discourses.

Applying topic models to fiction, however, brings new challenges.  \citet{jockers-13} trains a 500-topic model on a corpus of 4000 English-language novels.
Several issues emerge from this corpus. These are present in other contexts, but they are much more readily apparent in fiction.

\section{What is a Document?}

In most literature about topic models, the term ``document'' is used on the implicit assumption that users have things called documents.
In the canonical \abr{lda} journal article \citep{blei-03}, this word is used 143 times, but never defined.
In many cases, the meaning of a ``document'' is fairly clear: a news article, or a scientific abstract.
What was not clear in this earlier work was that this definition can be problematic, especially for documents longer than a few pages of text.

Treating novels as a single bag of words, for example, does not work.
Topics resulting from this corpus treatment are overly vague and lack thematic coherence.
We should not be surprised by this finding.
The assumption of a topic model is that the concentration of topics over a document is fixed and unchanging from the beginning of a document to the end.
Natural writing rarely fits the topic model assumption, and a novel that had no thematic variation over its entire length is unlikely to have been published.

We need to find a good segmentation into shorter contexts (in contrast to social media, which often needs to be combined into longer documents, c.f. Chapter~\ref{sec:twitter-strange}).
We assume that themes are expressed in different sections of a long document like a novel.
If a segmentation does a good job of identifying the boundaries between these sections, each resulting segment should have relatively few themes.
If a segmentation does not do a good job of identifying boundaries, we should see segments that contain more themes on average, because our segments combine fragments of multiple thematic segments.

 \citet{jockers-13} chooses to avoid relying on structural markers such as chapter divisions and divides novels into 1000-word chunks.
This treatment results in coherent, tightly focused topics that can be reasonably used as proxies for recognizable themes.

Although fixed-length segmentation is effective, it is not necessarily ideal.
\citet{algeehewitt2015paragraphs} compare varying fixed-length segmentations to segmentation based on paragraphs.
They evaluate the difference between treatments by measuring the concentration of topics in each segment of text after modeling.
The Herfindahl index is a measure of concentration in discrete probability distributions, calculated as the sum of the squared probabilities of each possible value:
\begin{align}
\text{Herfindahl}(P) = \sum_x P(x)^2.
\end{align}
For example, consider two distributions $P$ and $Q$ over a set of symbols $\{a, b, c, d, e\}$.
If $P$ has non-zero probability only on a single symbol $P(a) = 1.0$ and zero probability for all other symbols, the Herfindahl index of $P$ will be 1.0.
If $Q$ has uniform probability on all five symbols $Q(a) = Q(b) = ... = Q(e) = 0.2$, the Herfindahl index will be $5 \cdot \frac{1}{5} \cdot \frac{1}{5} = 0.2$.

When a corpus of 19\textsuperscript{th}-century novels is divided by paragraphs, the Herfindahl index over concentration of topics within each segment is consistently larger than the same index calculated when the same corpus is divided into evenly sized 200-word slices, implying that the distribution over topics for each segment is more focused on a smaller number of topics.
Setting the slice size to the average length of paragraphs in the corpus, eighty-two words, increases the Herfindahl concentration metric, but the resulting value is still smaller than the value based on paragraphs.
This result is reassuring, in that it suggests that paragraphs do indeed have some consistent meaning, at least in this collection of 19\textsuperscript{th}-century novels.

%\jbgcomment{We've not talked about twitter anywhere else, I think it would be a good example here}

% Comparison with Twitter?

\section{People and Places}

Because most works of fiction are set in imaginary worlds that have no existence outside the work itself, they are often characterized by words such as character names that are extremely frequent locally but never occur elsewhere.
This word co-occurrence pattern is problematic for topic models because they can be thought of as machines for finding groups of words that occur frequently together and not in other contexts.
Character names are---by that criterion---a perfect topic.
Modeling these documents can result in topics that are essentially lists of character names.

As an example, consider a model of 14 novels by Charles Dickens.
The top 20 words from a selection of topics ($K=50$) are shown in Table \ref{tbl:dickens-with-names}.
Upper-case letters are not reduced to lower-case in order to emphasize the presence of proper names.
Several topics are dominated by capitalized names, with individual novels clearly identifiable: Topic 4 is {\em Oliver Twist}, Topic 5 is {\em Nicholas Nickleby}, Topic 6 is {\em The Pickwick Papers} and Topic 7 is {\em A Tale of Two Cities}.
In fact, exactly half of the distinct words in the top 20 words for all topics are capitalized, and almost all of these are proper names.

Focusing on characters is not always uninformative, and can in some cases highlight structure within works.
Topics 1--3 all refer primarily to {\em Bleak House} (with the exception of {\em Scrooge}), but focus on different interlocking subplots.
The first focuses on Lady Dedlock, the second on Mr. Jarndyce and his two wards, Richard and Ada, and the third on the investigations of the detective Mr. Bucket.
The plot centers around the revelation of the connections between these apparently unrelated groups.

\begin{table}[htp]
\caption{Sample topics from Charles Dickens novels, without removal of character names (ordered manually).}
\begin{center}
\rowcolors{2}{gray!25}{white}
\begin{tabular}{cp{10cm}}
  \hline
  \rowcolor{gray!50}
  \hline
  Topic & Terms \\
  \hline \hline
  1 & Lady Leicester Scrooge Dedlock Rouncewell ladyship Wold
      Chesney Ghost Volumnia Christmas Tulkinghorn
    family Spirit Baronet nephew Rosa Scrooge's housekeeper Lady's \\
  2 & Richard Jarndyce guardian Ada Charley Caddy dear Skimpole Miss Summerson Esther Jellyby miss Vholes Kenge Woodcourt quite myself Guppy Chancery \\
  3 & says George Bucket Snagsby Guppy returns Smallweed Bagnet comes Tulkinghorn looks takes trooper does makes friend goes asks cries Chadband \\
  4 & Oliver replied Bumble Sikes Jew Fagin boy girl Rose Brownlow dear gentleman Monks Noah doctor Giles Dodger lady Nancy Bill \\
  5 & Nicholas Nickleby Ralph Kate Newman replied Tim Mulberry Mantalini Creevy brother N oggs Madame Gride Linkinwater Smike Arthur rejoined Wititterly Ned \\
  6 & Pickwick Winkle replied Tupman Wardle gentleman Snodgrass Pickwick's Perker fat boy Bardell dear Jingle inquired Fogg Dodson friends friend lady  \\
  7 &  Lorry Defarge Doctor Manette Pross Carton Darnay Madame Lucie Monseigneur Cruncher Jerry Stryver prisoner Charles Monsieur Tellson's Marquis father Paris \\
  8 & coach uncle gentleman lady box coachman gentlemen landlord get London guard inside horses waiter boys mail passengers large better hat \\
  9 & street door streets windows houses room window few iron walls wall rooms dark within shop doors corner small stood large  \\
  10 & money letter paper business read pounds papers five hundred office thousand clerk paid years pen next law desk letters week \\
  \hline
\end{tabular}
\end{center}
\label{tbl:dickens-with-names}
\end{table}%


\citet{jockers-13} approaches this problem by constructing a stopword list that removes all character names before modeling.
There are many ways to construct such lists.
Lists of common names are a good start, but may not be aligned with a specific corpus.
Some languages mark proper names with orthographic conventions like capitalization, but these tend to be noisy.
A useful heuristic in English is to identify terms that appear capitalized in more than 90\% of instances.
Even then, names that are also common words, such as {\em daisy} and the aforementioned Mr. Bucket, or words that appear capitalized for other reasons, such as {\em god}, may lead to unintended results.
Furthermore, some languages do not differentiate letter cases (Hebrew, Korean) and others use it for other purposes (all nouns in German).
Named-entity recognition tools scan text for patterns of language that indicate personal names, and may result in greater precision than simpler methods.
Nevertheless, there is no known way to avoid careful consideration of the meaning of words in context.

Novels describe people and places, but they are also created by people (authors) who are influenced by their cultural setting.
\citet{jockers-13b} perform a post-hoc analysis on Jockers' earlier 500-topic model to determine whether there is a connection between the use of specific topics and metadata variables such as author gender, author nationality, and year of publication.
They find that the concentration of many topics is strongly correlated with author gender, and that these correlations are statistically significant.
Such significance testing can be carried out by randomization and bootstrap tests.
Both methods create ``fake'' corpora that are similar to the real corpus but different in specific ways.
Randomization or permutation tests randomly shuffle the assignment of labels (such as author gender). If an observed correlation between a topic and an external variable is within the range of the correlations generated by randomly assigning documents to labels, there is little statistical evidence that that observed correlation is meaningful.
Bootstrap tests preserve the relationship between documents and metadata variables, but resample documents with replacement.
This test indicates whether a result depends on the presence or absence of a specific document.
If there is wide variation between randomly generated corpora, the observed correlation may be the result of unusual outliers rather than a consistent pattern.

While the use of statistical hypothesis testing methods is potentially valuable in the context of large-scale distant reading, a literary analysis is not---and should not be---like a clinical trial.
It is important to note some differences between their use in a scholarly context and their use in more typical scientific studies.
First, the presence of unusual outliers or singular examples can in fact be a positive result.
The suggestion that a particularly work may be radically different from supposedly similar examples could be the beginning of a new perspective.
At the very least, it can identify editing and curation issues.
Second, a critical variable in an analysis of statistical significance is sample size.
Unlike a designed experiment, this sample size is usually not within our control: we have the literature that we have.
Finally, it is vitally important to avoid the impulse to treat a significance score as a binary valid/invalid result.
If numeric scores should be used at all, they should be presented as a ``level of support'' given the documents that are available.
Humanists may also be fundamentally more comfortable with dubious hypotheses: an observed association with a 10\% chance of being purely random could still be a very strong result.

As an example, \citet{jockers-13b} evaluate an intriguing hypothesis, that a topic about religious foundations ({\underline{Convents and Abbeys}) is used more by unknown authors\footnote{Authors unknown to modern scholarship, not authors publishing under known pseudonyms.} than by either (known) male or female authors.
The conjecture is authors were choosing to remain anonymous in order to write about politically and religiously touchy subjects.
This correlation, however, showed large variability under a bootstrap test, and indeed one of the supposedly anonymous works turned out to be an abridgment of an Anne Radcliffe novel.
The pattern is still present without the effect of these works, but there is not a clear and undeniable association between anonymity and the questioning of religious authority.

In some cases fiction is set in the context of real places.
\citet{tangherlini-13} look at nested models of sub-corpora within Danish literature in a way that highlights connections between real-world events and cultural movements and fictional echoes.
Their method, which they describe as a topical ``trawl line,'' uses a user-specified sub-corpus as a query and then searches the remainder of the corpus for works that match to that query.
As examples, they find works influenced by the translation of Charles Darwin into Danish, works influenced by the ``Modern Breakthrough'', and works influenced by folklore and regional literature.


\section{Beyond the Literal}

One of the hallmarks of fiction and literature is the use of figurative language.
It is not obvious that unintelligent machines with no cultural understanding would have any ability to process such metaphors. However, \citet{rhody-12} demonstrates on a corpus of poetry that although topics do not represent symbolic meanings, they are a good way of detecting the concrete language associated with repeated metaphors.

Specifically, Rhody explores a corpus of 4500 poems that describe works of art (or {\em ekphrastic} poems).
She trains a sixty-topic model, and highlights several particularly interesting topics.
One of these topics places high probability on {\em night, light, moon, stars, day, dark, sun, sleep, sky, wind, time, eyes, star, darkness, bright}.
The apparent meaning of the topic is clear, and well summarized by the single top word: night.
But Rhody finds that when she explores the {\em context} of this topic, the poems are all using a consistent metaphor relating night and sleep to death.
The concept of death does not appear in the top words---poets are not addressing the issue directly.
Nevertheless, the model has identified an example of non-literal, figurative language even though, because it is grounded in the actual words, it has no ability to represent what the poets actually mean.
The model is able to do this because the poets are using a consistent ``surface'' language to represent a consistent metaphor.
The metaphor is not detectable directly, but a poet's use of a metaphor has a signature that is observable.

Rhody highlights a second topic that provides an example of a different type of non-literal meaning.
This topic places high probability on {\em death, life, heart, dead, long, world, blood, earth, man, soul, men, face, day, pain, die}.
Unlike the previous topic, the topic directly references death and life, but it also lacks what Rhody calls the ``unambiguous comprehensibility'' of the {\em night} topic.
But examining the context of poems that contain the topic reveals a different pattern.
These poems have a consistent {\em form} that Rhody describes as elegiac.
She writes that ``Paul Laurence Dunbar's `We Wear the Mask' never once mentions the word `death', the discourse Dunbar draws from to describe the erasure of identity and the shackles of racial injustice are identified by the model as drawing heavily from language associated with death, loss, and internal turmoil---language which `The Starry Night' indisputably also draws from''.

\section{Comparison to Stylometric Analysis}

In addition to discussing what researchers have done in literary analysis with topic models, it is useful to consider how other technologies have been used in the same setting.
One of the most established applications of computation in the study of literature is stylometry, or more specifically the question of authorship attribution~\citep{juola2006authorship}.
It is illustrative to contrast the goals and methods of stylometry with those of topic modeling.

The critical insight of modern stylometry is that it is easy for authors to shift the focus of their work, but much more difficult to alter the semi-conscious style of their language~\citep{mosteller1964inference}.
The implication is that content-bearing words, such as nouns and adjectives, are a relatively poor indicator of authorship or at least authorial style, while functional words, such as determiners, conjunctions, and prepositions, carry more information about authorship.
Therefore, measures such as Burrows' delta \citep{burrows2002delta} restrict attention to the most frequent words in a corpus.

The contrast to topic modeling is clear: stylometric analysis focuses on frequent, low-information words and ignores content-bearing words, while topic modeling generally does the exact opposite.
We generally remove high frequency words using a stop list, and in fiction go even further in removing words that are overly distinctive of a particular work.
An assumption of topic modeling is therefore that the goal is to find thematic components that are {\em not} specific to one author, but rather exhibit themselves, with more or less variation, across multiple works.
Where stylometry seeks to see past what authors are saying and focus on how they are saying it, a use of topic modeling is to find instances where different authors are writing about the same thing.

\section{Operationalizing ``Theme''}

The use of topic modeling in the study of literature has been beneficial both for humanities scholars and for machine learning researchers.
For scholars, these models offer the possibility of a more precise approach to concepts that have traditionally been vague and impressionistic, such as theme, genre, and motif.
At the same time, and somewhat paradoxically, literary documents present such a radically different mode of language than news articles or scientific publications that they lead us to question the apparent precision of statistical approaches.

Topic models provide a way of operationalizing the concept of distant reading.
\citet{moretti2013operationalizing} defines this term as ``Taking a concept, and transforming it into a series of operations''.
He attributes this definition to \citet{bridgman1927logic},  who introduces the term in the context of measurement in physics: ``To find the length of an object, we have to perform certain physical operations. The concept of length is therefore fixed when the operations by which length is measured are fixed: that is, the concept of length involves as much as and nothing more than the set of operations by which length is determined''.
While topic models are an imperfect tool for measuring theme in literature, they do provide a much more powerful approximation of theme than anything that we have had previously.

But applying statistical models to literature also brings forward a series of challenges that highlight the amount of human interpretive work that must go into successful topic modeling.
Literary documents are of varied lengths, describe self-contained imaginary worlds, and are suffused with symbolic language.
We can address these issues through corpus curation and through interpretive reading of models, but in doing so we must necessarily confront the fact that we are not simply applying Bridgman's fixed set of operations.

\section{Summary}

Topic models cannot by themselves study literature, but they are useful \textit{tools} for scholars studying literature.
Models provide a distinct perspective that can call our attention to connections across different parts of a corpus that might not be obvious from close reading. Literary concepts are complicated, but they often have surprisingly strong statistical signatures. Models can still be useful in identifying areas of potential interest, even if they don't ``understand'' what they are finding. At the same time, fiction presents a challenge to modeling practices because each fictional work deliberately creates its own closed world---a world whose characters and settings are so vivid that they can overshadow more subtle connections across works. Addressing these challenges can serve as an invitation to think deeply about words and their contextual meanings.

Just as topic models provide a methodology for analyzing the creative, diverse \oe{}uvre of authors and the emotions and thoughts of fictional characters, topic models can also help us build insights of real people.
Thanks to social media, we have a wealth of information about people sharing their thoughts and views online.  Topic models can help us use these data to better capture emotion,
beliefs, and relationships.  The next chapter discusses how topic models can
understand these messy, interesting properties of text.
