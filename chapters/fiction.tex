
\chapter{Fiction and Literature}

\label{ch:fiction}

We value literature because it is one of the best ways to capture the spirit of an age, and the experiences of those who lived through it. But standard methods of close reading require focus and thorough interpretation. As a result, scholars are often left trying to make large-scale arguments about the history of literature from small-scale evidence. And this small-scale exploration is not randomly selected: the same small canon is studied in detail while the vast proportion make up the ``great unread'' \citep{moretti-00}, works that are never studied.

\section{The Role Topic Models Play in the Humanities}

As a response, Moretti theorized an alternative ``distant reading''  \citep{moretti-13} that uses computer-assisted methodologies. Topic modeling has emerged as a central tool in distant reading, as a way to organize our reading of large scale patterns.

Topic analysis, viewed as a way of identifying repetitions of language or discourse through multiple works, resonates with many more familiar approaches to the study of literature.
At the broadest scale, to define a genre or a literary period is to separate a corpus into sections based on some observable criterion.
We posit a ``gothic'' literature characterized by atmospheric descriptions of castles, or a ``cyberpunk'' literature characterized by conflicted relationships with information technology.
At a smaller scale, themes or tropes reappear in different contexts.
At the most detailed level, scholars identify repeated phrases, such as the descriptive epithets used in Homeric oral poetry.

Statistical topic analysis has a similar goal, but pursues it through different means.
Rather than rigid boundaries specified by date of publication or nationality, algorithms identify genre through the repeated words that form the traces of those themes.
Topics do not represent themes themselves, but rather identify the implicit statistical regularities in word use brought about by the presence of genres, themes, and discourses.


Applying topic models to fiction, however, brings new challenges. Jockers \citep{jockers-13} trains a 500-topic model on a corpus of 4000 English-language novels.

Several issues emerge from this corpus. These are not unheard of in other contexts, but they are much more readily apparent in fiction.

\section{What is a Document?}

 Treating novels as a single bag of words does not work. We need to find a good segmentation into shorter contexts.

Paragraph-based segmentation [currently trying to track down a reference to work done recently by Stanford Literary Lab]

% Comparison with Twitter?

\section{People and places}

Because works of fiction are set in imaginary worlds that have no existence outside the work itself, they are often characterized by words such as character names that are extremely frequent locally but never occur elsewhere. Modeling these documents can result in topics that are essentially lists of character names.

Jockers and Mimno \citep{jockers-13b} analyze the earlier 500-topic model to determine whether there is a statistically significant connection between the use of specific topics and metadata variables such as author gender, author nationality, and year of publication.

Tangherlini and Leonard \citep{tangherlini-13} look at nested models of sub-corpora within Danish literature.

\section{Beyond the Literal}

One of the hallmarks of fiction and literature is the use of figurative language.
It is not obvious that unintelligent machines with no cultural understanding would have any ability to process such metaphors. However, Rhody \citep{rhody-12} demonstrates on a corpus of poetry that although topics do not represent symbolic meanings, they are a good way of detecting the concrete language associated with repeated metaphors.

Specifically, Rhody explores a corpus of 4500 poems that describe works of art (or {\em ekphrastic} poems).
She trains a 60-topic model, and highlights several particularly interesting topics.
One of these topics places high probability on {\em night, light, moon, stars, day, dark, sun, sleep, sky, wind, time, eyes, star, darkness, bright}.
The apparent meaning of the topic is clear, and well summarized by the single top word: night.
But Rhody finds that when she explores the {\em context} of this topic, the poems are all using a consistent metaphor relating night and sleep to death.
The concept of death does not appear in the top words --- poets are not addressing the issue directly.
Nevertheless, the model has identified an example of non-literal, figurative language even though, because it is grounded in the actual words, it has no ability to represent what the poets actually mean.
The model is able to do this because the poets are using a consistent ``surface'' language to represent a consistent metaphor.
The metaphor is not detectable directly, but a poet's use of a metaphor has a signature that is observable.

Rhody highlights a second topic that provides an example of a different type of non-literal meaning.
This topic places high probability on {\em death, life, heart, dead, long, world, blood, earth, man, soul, men, face, day, pain, die}.
Unlike the previous topic, the topic directly references death and life, but it also lacks what Rhody calls the ``unambiguous comprehensibility'' of the {\em night} topic.
But examining the context of poems that contain the topic reveals a different pattern.
These poems have a consistent {\em form} that Rhody describes as elegiac.
She writes that ``Paul Laurence Dunbar's 'We Wear the Mask' never once mentions the word 'death,' the discourse Dunbar draws from to describe the erasure of identity and the shackles of racial injustice are identified by the model as drawing heavily from language associated with death, loss, and internal turmoil --- language which 'The Starry Night' indisputably also draws from.''

