\chapter{Multilingual Data and Machine Translation}
\label{ch:mt}

\jbgcomment{This introduction needs to be improved.  I think you should start with how many datasets are multilingual, then discuss how topic models help in this case, and then give an overview of the chapter. \yhcomment{fixed.}}

So far, we have been focusing on monolingual topic models and their applications. In fact, multilingual topic models have also been developed to analyze and understand the corpus in multiple langauges, and statistical machine translation~(\textsc{smt}) is one of the major applications.

Given a text input in one language (source language), statistical machine translation tries to find a similar piece of text in another language (target language). Modern machine translation systems~\citep{koehn-09} use millions of training examples to learn the translation rules and apply these rules on the test data. While the translation rules are learned in local context, these systems work best when the training corpus has consistent global context or the same genre (e.g., sports, business) from a similar style (e.g., newswire, blog-posts).  These are called \emph{domains}.  

Translations within one domain are better than translations across domains since they vary dramatically in their word choices and style.  A correct translation in one domain may be inappropriate in another domain.  For example, ``\begin{CJK*}{UTF8}{gbsn}潜水\end{CJK*}'' in \underline{sports} domain usually means ``underwater diving'', but in \underline{social media} domain, it means a non-contributing ``lurker''. To avoid such translation errors caused by the domain change, domain knowledge is needed to train translation systems that are robust to such systematic variation in the training set, which are said to exhibit \emph{domain adaptation}.

To Train such \textsc{smt} systems with domain adaptation, early efforts focus on building separate models given the hand-labeled domains~\citep{foster-07,matsoukas-09,chiang-11}. However, this setup is at best expensive and at worst infeasible for large data.  
Topic models provide a promising solution where domains can be automatically discovered. Each extracted topic is treated as a soft domain. \footnote{Henceforth we will use the term ``topic'' and ``domain'' interchangeably: ``topic'' to refer to the concept in topic models and ``domain'' to refer to \textsc{smt} corpora.} Thus the normal monolingual topic models on the source documents only have been applied to extract domain knowledge for machine translation~\citep{Eidelman-12}. 

However, the source language the and target language can complement each other to build up more accurate topic models. For example, if we only know the Chinese phrase ``\begin{CJK*}{UTF8}{gbsn}潜水\end{CJK*}'', it is hard to decide whether it is a \underline{sport} domain or it is a \underline{social media} domain. However, with the help of the aligned English translation ``luker'', it is easy to identify the ``social media'' domain. Thus multilingual topic models~\citep{mimno-09,boyd-graber-10} have been applied to extract domain knowledge for machine translation~\citep{hu-14}. 

This chapter first reviews the basic components of statistical machine translation, then introduces how to apply the monolingual and multilingual topic models in domain adaptation to improve each component of the statistical machine translation systems.

\section{Statistical Machine Translation}

Statistical machine translation casts machine translation as a probabilistic process~\citep{koehn-09}. Here we briefly introduce the standard phrase translation model introduced in~\citep{koehn-03}. Given the source sentence $\mathbf{f}$, the best translation in target language $\mathbf{e}_\texttt{best}$ is modeled as,
\begin{equation}
\mathbf{e}_\texttt{best} = \textbf{argmax}_\mathbf{e} p(\mathbf{e}|\mathbf{f}) = \textbf{argmax}_\mathbf{e} p(\mathbf{f}|\mathbf{e}) p (\mathbf{e})
\end{equation}
which is split to a \textit{translation model} $p(\mathbf{f}|\mathbf{e})$ and a \textit{language model} $p (\mathbf{e})$.

The source sentence $\mathbf{f}$ is segmented into multiple source phrases $\bar{f}_n$ during the decoding, which are translated to a set of target phrases $\bar{e}_n$. Thus the translation probability $p(\mathbf{f}|\mathbf{e})$ can be further decomposed to the phrase translation probability $p(\bar{f}_n | \bar{e}_n)$. Besides, the target phrases may need to be \textit{reordered} to get the best translation result, and this part is captured by a relative distortion probability distribution $d(a_n - b_{n-1})$, where $a_i$ denotes the start position of the source phrase that was translated to the $n$th target phrase, and $b_{n-1}$ denotes the end position of the source phrase translated into the $(n-1)$th target phrase. As a result, the translation model can be decomposed as,
\begin{equation}
p(\mathbf{f}|\mathbf{e}) = \prod_{n} p(\bar{f}_n | \bar{e}_n) d(a_n - b_{n-1})
\end{equation}

In phrase-based \textsc{smt}, the phrase probability $p(\bar{f}_n | \bar{e}_n)$ can be further estimated by combining lexical translation probabilities of words contained in that phrase~\citep{koehn-03}, which is normally referred as \textit{lexical weighting}. Lexical conditional probabilities $p_w(f|e)$ are maximum likelihood estimates from relative lexical frequencies,
\begin{equation}
\label{eq:lexical_prob}
p_w(f|e) = \textstyle \slfrac{c(f, e)}{\sum_f{c(f, e)}}
\end{equation}
where $c(f, e)$ is the count of observing lexical pair $(f, e)$ in the training dataset. Given a word alignment $a$, the lexical weight for this phrase pair $p_w(\bar{f} | \bar{e}; a)$ is the normalized product of lexical probabilities of the aligned word pairs within that phrase pair:
\begin{equation}
\label{eq:phrase_prob}
p_w(\bar{f} | \bar{e}; a) = \prod_{i} \frac{1}{\{|j | (i, j) \in a\}|} \sum_{\forall (i,j) \in a} p_w(f_i | e_j)
\end{equation}
where $i$ and $j$ are the word positions in target phrase $\bar{e}$ and source phrase $\bar{f}$ respectively.

Next we introduce how to apply topic models to improve translation models, language models and reordering models respectively.

\jbgcomment{Should this really be a new section? \yhcomment{Restructured. Let me know what you think now.}}

\section{Topic Models in Translation Models}

To Train such \textsc{smt} systems with domain adaptation, early efforts focus on building separate models based on the hand-labeled domains~\citep{foster-07,matsoukas-09,chiang-11}. For example, for all the training examples that are labeled as \underline{sports} domain, one translation model is trained. As a result, in any test example that is labeled as \underline{sports}, ``\begin{CJK*}{UTF8}{gbsn}潜水\end{CJK*}'' is always translated to ``underwater diving'', and the probability of translating ``\begin{CJK*}{UTF8}{gbsn}潜水\end{CJK*}'' to ``lurker'' is zero. In fact, such hard domain labels are not only expensive and time consuming to obtain, but also unsmoothed and sensitive to labeling erros. 

Unlike the manual hard domain labels, topic models provide a promising solution for automatically discovering the soft domain assignments. While the topic-word distributions generated by topic models show what each topic (domain) is about, the document-topic distributions provide such soft domain assignments for each document. Let's continue to use the previous example and assume there are two topics \underline{sports} and \underline{social media}. Given a test example that is most likely about \underline{sports}, it may have a soft domain distribution as $99\%$ for \underline{sports} domain and $1\%$ for \underline{social media}. These automatically obtained soft domain labels are well smoothed, and they are not only cheap to obtain but also much more robust to topic errors. Next, we introduce the details about applying monolingual and multilingual topic models to improve translation models~\citep{Eidelman-12,hu-14}.

\jbgcomment{Give an example of Chiang's hard domain assignment and then give an example of how topic models can do this using ``soft'' assignment.  Then go into the mathematical details. \yhcomment{added.}}

\subsection{Translation Domain Adaptation with Topic Models}

Topic models take the number of topics $K$ and a collection of documents as input, where each document is a bag of words. They output two distributions: a distribution over topics for each document $d$; and a distribution over words for each topic. If each topic defines a \textsc{smt} domain, the document's topic distribution is a soft domain assignment for that document.

%Given the soft domain assignments, \citet{Eidelman-12} extract lexical weighting features conditioned on the topics, optimizing feature weights using the \emph{Margin Infused Relaxed Algorithm}~\cite[\textsc{mira}]{Crammer:2006}.  The topics come from source documents \emph{only} and create topic-specific lexical weights from the per-document topic distribution $p(k|d)$, which is used to smooth the expected count $\hat{c}_{k}(f,e)$ of a word translation pair under topic $k$,

\paragraph{Topical Lexical Features}

\citet{Eidelman-12} use such soft domain assignments to build up translation models. The document-topic distribution $p(k|d)$ is used to smooth the expected count $\hat{c}_{k}(f,e)$ of a word translation pair under topic $k$,
\begin{align}
\textstyle \hat{c}_{k}(f,e) = \sum_{d}{p(k|d)c_d(f,e)},
\end{align}
where $c_d(\bullet)$ is the number of occurrences of the word pair in document $d$.  The lexical probability conditioned on topic $k$ is the unsmoothed probability estimate of those expected counts
\begin{align}
\label{eq:lexical_prob_k}
\textstyle p_w(f|e;k) = \hat{c}_{k}(f,e) / \sum_f{\hat{c}_{k}(f,e)},
\end{align}
from which we can compute the lexical weight of this phrase pair
$p_w(\bar{f}|\bar{e};a, k)$ given a word alignment $a$\citep{koehn-03}:
\begin{align}
\label{eq:phrase_prob_k}
p_w(\bar{f} | \bar{e};a, k) = \prod^{n}_{i=1} \frac{1}{\{|j | (i, j) \in a\}|} \sum_{\forall (i,j) \in a} p_w(f_i | e_j; k)
\end{align}
where $i$ and $j$ are the word positions in target phrase $\bar{e}$ and source phrase $\bar{f}$ respectively. Comparing Equation~\ref{eq:lexical_prob_k} against Equation~\ref{eq:lexical_prob}, and Equation~\ref{eq:phrase_prob_k} against Equation~\ref{eq:phrase_prob}, we can clearly see how the topics are encoded as soft domains and how these topics influence the probability.

For a test document $d$, the document topic distribution $p(k | d)$ is inferred based on the topics learned from training data. The lexical weight feature of a phrase pair $(\bar{f}, \bar{e})$ is,
\begin{align}
\textstyle f_{k}(\bar{f}|\bar{e})=-\log\left\{{p_{w}(\bar{f}|\bar{e};k)\cdot p(k|d)}\right\},
\end{align}
a combination of the topic dependent lexical weight and the topic distribution of the document, from which we extract the phrase.

In fact, \citet{Eidelman-12} introduces the two direction topic-adapted probabilities instead of a single direction: $p_w(\bar{f} | \bar{e};a, k)$ and $p_w(\bar{e} | \bar{f};a, k)$. This is equivalent to introduce $2K$ new word translation tables, thus it results in introducing $2K$ lexical weight features $f_{k}(\bar{f}|\bar{e})$ and $f_{k}(\bar{e}|\bar{f})$. These topic-adapted features are combined with the other standard \textsc{smt} features including the standard $f(\bar{f}|\bar{e})$ and $f(\bar{e}|\bar{f})$, and the feature weights are optimized through using the \emph{Margin Infused Relaxed Algorithm}~\cite[\textsc{mira}]{Crammer-06}. 

These adapted features allow us to bias the translations according to the topics. For example, if topic $k$ is dominant in a test document, the feature $f_k(\bar{f} | \bar{e})$ will be large, which may bias the decoder to a translation that has small value of the standard feature $f(\bar{f}|\bar{e})$. In addition, combining the adapted features with the standard features makes this model more flexible. For a test document with less clear topics, the topic distribution will tend toward being fairly uniform. In this case, the topic features will contribute less to the translation results and the standard features will dominate the translation results.

\paragraph{Topical Lexical and Phrasal Features}

\citet{hasler-12} also apply topic models for domain adaptation to \textsc{smt} in a similar framework as \citet{Eidelman-12}, except they introduce different features, which they call sparse word pair features and phrase pair features. The topics on source documents are integrated as a source side trigger for a particular word pair or phrase pair as sparse features. For  example, given a word $w_f$ with topic $k$, for an aligned word pair $(w_f, w_e)$ which is observed $c$ times in the aligned sentence pair, the sparse word pair feature $wp$ with topics is represented as $wp\_k\_w_f \sim w_e = c$, while the original word pair feature is $wp\_w_f \sim w_e = c$. The topic phrase pair features are also defined in a similar way: given an aligned phrase pair $(p_f, p_e)$ with count $c$ in the same sentence, the sparse phrase pair feature $pp$ with topics is represented as $pp\_k\_p_f \sim p_e = c$. Both the phrase pair and word pair features are extracted from the aligned training sentence pairs, and then \textsc{mira} is used to learn the feature weights.

One difference in \citet{hasler-12} from \citet{Eidelman-12} is that they are using \emph{hidden topic Markov models}~\citep[\textsc{htmm}]{gruber-07} instead of \textsc{lda} to learn topics. While \textsc{lda} assumes that each word is generated independently in a document, \textsc{htmm} models the word topic in a document as a Markov chain where all words in a sentence are assigned with the same topic. \textsc{htmm} computes $p(z_n, \Phi_n | d, w_1, \cdots, w_N)$ for each sentence, where $z_n$ is the topic of sentence $n$ in document $d$, $w_1, \cdots, w_N$ are the words in the sentence $n$,  $\Phi_n$ is the topic transition between words. $\Phi_n$ is only non-zero at sentence boundaries. The advantage of using \textsc{htmm} is that each sentence gets the same topic assignment, thus the topic for each phrase pair in the aligned sentence is consistent and can be used for topical features directly. 

\jbgcomment{Contrast HTMM with LDA and why it might be better.  You should also explain HTMM a little more, as it's not discussed anywhere else. \yhcomment{added.}}

\paragraph{Topical Phrase Probability via Topic Mapping}

\citet{su-12} also apply \textsc{htmm} to \textsc{smt} based on a similar idea as \citet{Eidelman-12}. Given the bilingual translation training data without any specific domain information (referred as out-of-domain bilingual data), they incorporate the topic information from the source language into translation probability estimation, and decompose the phrase probability $p(\bar{e}|\bar{f})$ as,
\begin{align}
p(\bar{e}|\bar{f}) = \sum_{k_{out}} p(\bar{e}, k_{out} | \bar{f}) = \sum_{k_{out}} p(\bar{e} | \bar{f}, k_{out}) \cdot p(k_{out} | \bar{f})
\end{align}
where $p(\bar{e} | \bar{f}, k_{out})$ is the translation probability given the source side topic $k_{out}$, and $p(k_{out} | \bar{f})$ denotes the phrase probability in topic $k_{out}$.

Besides, \citet{su-12} assume that a monolingual corpus in the same domain as the test sentence(referred as in-domain monolingual data) is available. Thus they also apply \textsc{htmm} to estimate the in-domain topic $k_{in}$ and $p(k_{in} | \bar{f})$. 
However, the in-domain topics $k_{in}$ and the out-of-domain topics $k_{out}$ may not be in the same space, so \citet{su-12} introduce the topic mapping probability $p(k_{out} | k_{in})$ to map the in-domain topic to the out-of-domain topic:
\begin{align}
p(k_{out} | \bar{f}) = \sum_{k_{in}} p(k_{out} | k_{in}) \cdot \cdot p(k_{in} | \bar{f})
\end{align}
As a result, the final phrase probability can be refined as,
\begin{align}
p(\bar{e}|\bar{f}) = \sum_{k_{out}} \sum_{k_{in}} p(\bar{e} | \bar{f}, k_{out}) \cdot p(k_{out} | k_{in}) \cdot p(k_{in} | \bar{f})
\end{align}

Comparing with the approaches in \citet{Eidelman-12} and \citet{hasler-12}, this approach incorprates the topic information into the phrase probability directly, rather than through the word translation probability. The topics and topic mapping relationship between the training data and test data can be built offline, thus the whole process brings no additional burden to the translation system. 

\paragraph{Problems}

However, \citet{Eidelman-12,hasler-12,su-12} ignore a wealth of information that could improve topic models and help machine translation. They rely solely on monolingual source-side models. In contrast, machine translation uses inherently multilingual data: an \textsc{smt} system must translate a phrase or sentence from a \emph{source} language to a different \emph{target} language, so these approaches are ignoring available information on the target side that could aid domain discovery. This can be improved by introduction multilingual topic models, and the details are introduced in the next section.

\subsection{Multilingual Information for Domain Adaptation}
\label{sec:trans-multiling}

While the previous approaches focus on applying topic models on the monolingual source language of tranlsation data, the bilingual data indeed can complement each other to reduce topic ambiguity.  For example, ``\begin{CJK*}{UTF8}{gbsn}木马\end{CJK*}'' in a Chinese document can be either ``hobbyhorse'' in a \underline{children}'s topic, or ``Trojan virus'' in a \underline{technology} topic.  A short Chinese context obscures the true topic. However, these terms are unambiguous in English, revealing the true topic. From this example, we can see that that topic models on multilingual corpus do improve the quality of the extracted topics.

There are various ways to build up the multilingual topic models. Different languages can be connected on the word-level~\citep{boyd-graber-07,andrzejewski-09,hu-14:itm} or the document levels~\citep{mimno-09}. \citet{hu-14} further combine the two types of connections to build up multilingual topic models. Given the improved multilingual topic models, the extracted topics are further applied in the \textsc{smt} framework from \citet{Eidelman-12} to improve the machine translation results.

\paragraph{\bf Word-level Correlations}

\emph{Lexical information}, such as orthographic similarity~\citep{boyd-graber-09} and multilingual dictionaries~\citep{boyd-graber-10}, can be very helpful to induce better topics from multilingual corpora. For instance, tree-based topic models~\citep[\tlda{}]{boyd-graber-07,andrzejewski-09,hu-14:itm} incorporate the positive correlations between words in the same or different languages by encouraging words that appear together in a {\bf concept} to have similar probabilities given a topic. These concepts can come from WordNet~\citep{boyd-graber-10}, domain experts~\citep{andrzejewski-09}, or user constrains~\citep{hu-14:itm}. If these concepts are in the same language, it is the same backend model for interactive topic modeling introduced in Chapter~\ref{ch:viz}. However, when we gather concepts from bilingual resources, these concepts can connect different languages. For example, if a bilingual dictionary defines ``\begin{CJK*}{UTF8}{gbsn}电脑\end{CJK*}'' as ``computer'', we combine these words in a concept.

These concepts (positive correlations) are organized into a {\bf prior tree} structure. As Figure~\ref{fig:prior_trees} shows, words in the same concept share a common parent node, and then that concept becomes one of many children of the root node.  Words that are not in any concept---{\bf uncorrelated words}---are directly connected to the root node. Thus a topic becomes a distribution over all paths in this prior tree and each path is associated with a word. 

The probability of a path in a topic depends on the transition probabilities in a topic.  Each concept $i$ in topic $k$ has a distribution over its children nodes is governed by a Dirichlet prior: $\pi_{k,i} \sim \text{Dir}(\beta_{i})$.  Each path ends in a word (i.e., a leaf node) and the probability of a path is the product of all of the transitions between topics it traverses. Topics have correlations over words because the Dirichlet parameters can encode positive or negative correlations~\citep{andrzejewski-09}.

As a result, in order to sample a word $w_{dn}$ given a topic $z_{dn}$, a path $y_{dn}$ from the topic tree of topic $z_{dn}$ is sampled: we start from the root $n_0$ and first sample a child node $n_1$ of the root; if node $n_1$ is a concept node, we continue to sample a word node $n_2$ and generate the word assciated with $n_2$; if node $n_1$ is a word node already, we generate the word directly.

\jbgcomment{Give intuition, e.g. selecting a concept first and then you get the language-specific version. \yhcomment{added.}}

When this tree serves as a prior for topic models, words in the same concept are positively correlated in topics.
For example, if ``\begin{CJK*}{UTF8}{gbsn}电脑\end{CJK*}'' has high probability in a topic, so will ``computer'', since they share the same parent node. With the tree priors, each topic is no longer a distribution over word types; instead, it is a distribution over paths, and each path is
associated with a word type.  The same word could appear in multiple paths, and each path represents a unique sense of this word.

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{figures/correlations_tree-crop.pdf}
\vspace{-3mm}
\caption[Constructing prior tree from a bilingual dictionary]{An example of constructing a prior tree from a
  bilingual dictionary: word pairs with the same meaning but in
  different languages are concepts; a common parent node is created to
  group words in a concept, and then is connected to the root;
  uncorrelated words are connected to the root directly.}
\label{fig:prior_trees}
\end{figure}

\jbgcomment{Would be nice if the prior tree were multilingual.  Would also be good to give back pointer to the interaction section where it's also discussed \yhcomment{fixed.}}

\paragraph{\bf Document-level Alignments}

Lexical resources connect languages and help to guide the topics. However, these resources are sometimes brittle and may not cover the whole vocabulary. Aligned document pairs provide a more corpus-specific, flexible association across languages.

\citet{Landauer-1990} connect aligned documents in different languages by projecting both documents to a shared latent semantic indexing space. Similarly, polylingual topic models~\citep[\plda{}]{mimno-09} assume that the aligned documents in different languages share the same topic distribution and each language has a unique topic distribution over its word types.

Thus the generative process of polylingual topic model is as follows: given a document pair $(d_{l_1}, d_{l_2})$, we first sample a document-topic distribution $\theta_d$; for a document $d_{l_i}$ in language $l_i$, we then sample a topic $z_{dn}$ from $\theta_d$, and generate a word from topic $\phi_{z_{dn}, l_i}$ in language $l_i$. 

This connection between languages is flexible: instead of requiring the exact matching or translations on words and sentences, only a coarse document alignment is necessary, as long as the documents discuss the same topics, e.g., wikipedia articles in different languages. Such connection between languages is also helpful to infer more robust topics, since different languages can complement each other to reduce ambiguity. 

\jbgcomment{Give example of linked documents and where they come from.  Stress that they don't have to be translations. \yhcomment{fixed.}}

\paragraph{\bf Combine Words and Documents}

These two approaches are not mutually exclusive, however; they reveal different connections across languages. \citet{hu-14} bring existing tree-based topic models~(\tlda{}) and polylingual topic models~(\plda{}) together and create the polylingual tree-based topic model~(\ptlda{}) that incorporates both word-level correlations and document-level alignment information. 

To build up the prior tree structure, \citet{hu-14} consider two resources that correlate words across languages. The first is multilingual dictionaries, which match words with the same meaning in different languages together. The other is the word alignments extracted from aligned sentences in a parallel corpus. These relations between words are used as the concepts~\citep{Bhattacharya-2006} in the prior tree (Figure~\ref{fig:prior_trees}).

Given the prior tree structure, the generation of documents is a combination of \tlda{} and \plda{}.  For each aligned document pair $(d_{l_1}, d_{l_2})$, we first sample a distribution over topics $\theta_d$ from a Dirichlet prior $\text{Dir}(\alpha)$.  For each token in the aligned document $d_{l_i}$, we first sample a topic $z_{dn}$ from the multinomial distribution $\theta_d$, and then sample a path $y_{dn}$ along the tree of topic $z_{dn}$. Because every path $y_{dn}$ leads to a word $w_{dn}$ in language $l_{dn}$, we append the sampled word $w_{dn}$ to document $d_{l_{dn}}$ in language $l_{dn}$. 

If a flat symmetric Dirichlet prior is used instead of the tree prior, \plda{} is recovered; and if all documents are monolingual (i.e., with distinct distributions over topics $\theta$), \tlda{} is recovered. \ptlda{} connects different languages on both the word level (using the word correlations) and the document level (using the document alignments), thus it learns better topics by considering more information from both languages. 

\paragraph{\bf Topic Projection}

While the above approaches try to model the source and target languages simultaneously to extract topics, \citet{xiao-12} apply topic models on the source documents and target documents respectively to learn the document-topic distributions $p(k_f | d_f)$ and $p(k_e | d_e)$, and then estimate the phrase-topic probability $p(\bar{e}, k_f | \bar{f})$ and $p(\bar{e}, k_e | \bar{f})$ respectively. They further compute the topic simiality scores between the phrase topic distribution and document topic distribution as features for decoding to improve \textsc{smt} results.

For test data, they estimate the document-topic distribution $p(k_f | d_f)$ for source document only, and compute the topic similarity between the document-topic distribution $p(z_f|d_f)$ and the phrase-topic distribution $p(\bar{e}, z_f|\bar{f})$ as one similarity feature for decoding,
\begin{align}
\texttt{Similarity}(p(z_f|d_f), p(\bar{e}, z_f|\bar{f})) = \sum_{k=1}^{K} (\sqrt{p(z=k_f|d_f)} - \sqrt{p(z=k_f, \bar{e}|\bar{f})})^2
\end{align}

Similarly, they also consider a second similarity feature $\texttt{Similarity}(p(z_f|d_f), p(\bar{e}, z_e|\bar{f}))$. However, the source topics $z_f$ may not be in the same space as the target topics $z_e$. Thus they learn the topic projection probability $p(k_f | k_e)$ by normalizing the co-occurrence count in the aligned training sentences, then the target phrase-topic probability $p(\bar{e}, z_e|\bar{f})$ is projected to the source topic space as,
\begin{align}
P(p(\bar{e}, z_e|\bar{f})) = p(\bar{e}, z_e|\bar{f}) \cdot p(k_f | k_e)
\end{align}
Thus the second feature $\texttt{Similarity}(p(z_f|d_f), P((\bar{e}, z_e|\bar{f})))$ is computed. 

This topic projection idea is similar to the topic mapping by \citet{su-12}, but it is applied between the source language and the target language. Compared to the lexical features in \citet{Eidelman-12} and \citet{hu-14}, \citet{xiao-12} introduce a new framework to apply topic information on phrase directly and present two topic similarity features for decoding. These two approaches can be combined to further improve \textsc{smt}.

\jbgcomment{How does this work in smt? \yhcomment{Rewrote this section to make it more clear.}}


\section{Topic Models in Language Modeling}

\jbgcomment{Would be good to give the basic intution of what an LM is. \yhcomment{added.}}

A critical component of machine translation systems is the language models, which provide local constraints and preferences to make translations more coherent. A language model describes the probability of a word $w$ occurring given the previous context words, which is also mentioned as the history $h$. We have introduced the application of language models in information retrieval in Chapter~\ref{sec:ir-lm}. In fact, it also helps to choose the correct or more proper word during the statistical machine translation. For example, the English translation ``I am going home'' is preferred than ``I am going house''. 

Domain adaptation for Language models~\citep{Bellegarda-04,wood-09} uses extra knowledge to adjust this probability $p(w|h)$ to reflect the content change, which is an important avenue for improving machine translation. As \citet{Bellegarda-04} points out that `` an adaptive language model seeks to maintain an adequate representation of the current task domain under changing conditions involving potential variations in vocabulary, syntax, content, and style''.

Topics from topic models can be one of the resources to provide such knowledge for language model adaptation. For example, the Chinese phrase ``\begin{CJK*}{UTF8}{gbsn}很多粉丝\end{CJK*}'' is translated to ``a lot of vermicelli'' in a \underline{food} domain, but means ``a lof of fans'' in an \underline{entertainment} domain. Such ambiguity can be reduced by considering the topics (domain knowledge)from topics. If the \underline{entertainment} topic is extracted based on the previous context, this Chinese phrase will be translated to ``a lof of fans'' without any ambiguity. Next, we introduce the details about applying topic models for language model adaptation.

\jbgcomment{Need to explain why language models are useful in translation.  E.g., give two translation options and show how LM adjust.  Then motivate how TM can help with that. \yhcomment{fixed.}}


\subsection{Monolingual Topic Models for Language Model Adaptation}

Early work such as \citet{Clarkson-1997,Seymore-1997,Kneser-1997,Iyer-1999} focus on partitioning the training data to multiple topic-specific subsets and building up language models for each subset. Then the topic-specific language models $p_k(w|h)$ are linearly combined with a general language model $p_g(w|h)$ built from all training data as Equation~\ref{eq:linear_lm}. The weights $\lambda_k$ can be tuned based on the topics of the test documents.
\begin{align}
\label{eq:linear_lm}
p_\textrm{adapted}(w|h) = \sum_k \lambda_k p_k(w|h) + \lambda_g p_g(w|h)
\end{align}

\citet{Seymore-1998} further identify the most qualified topic for each word in the vocabulary and choose a topic-specific language model or the general language model. The intuition is that the general language model provides the most reliable estimation for general words, and the topic language model estimates the probability more accurately for topic words. As a result, they split the vocabulary words into three groups: the general subset, on-topic subset and off-topic subsets. For general subset and off-topic subset, the general language model provides the word probability; and the topic-specific language model provides the word probability for the on-topic subset.

While these early attempts introduce the topics to improve the language models, these topic-specific language models are just like the traditional n-gram models, which can model the limited history. Besides, these models assume each document or history belongs to exactly one topic cluster.

To fix these problems, models with topic mixtures, such as  \emph{Latent Semantic Analysis}~\citep[\textsc{lsa}]{deerwester-90} and its probabilistic interpretation---probabilistic latent semantic indexing~\citep[\textsc{plsi}]{hofmann-99}, have been introduced to learn large-span language models~\citep{Bellegarda-1997,Coccaro-1998,Gildea-1999}. \citet{Gildea-1999} decomposes the language model based on topics,
\begin{align}
\label{eq:plsi_lm}
p(w|h) = \sum_k p(w|k) p(k|h)
\end{align}
where the topics are learned from the training corpus by optimizing the log probability,
\begin{align}
l(\theta; N) = \sum_w \sum_d n(w,d) \log \sum_k p(w|k) p(k|d)
\end{align}
where $d$ is the training documents, and $n(w,d)$ is the word frequency of $w$ in document $d$. $p(w|k)$ and $p(k|d)$ are learned through the EM algorithm. For test documents, they fix $p(w|k)$ to estimate $p(k|h)$ and then compute $p(w|h)$ using Equation~\ref{eq:plsi_lm}. 

These approaches of applying topic models to language models are very similar to the document language modeling for information retrieval, which have been introduced in Chapter~\ref{sec:ir-lm}. However, unlike information retrieval, two different languages are involved in the process of \textsc{smt}, and they can complement each other to learn more accurate topics. Next, we introduce the multilingual topic models for language model adaptation.

\subsection{Multilingual Topic Models for Language Model Adaptation}

As we explained in Section~\ref{sec:trans-multiling}, the information from different languages can complement each other to extract better topics. In order to introduce multilingual information to topic models for language model adaptation, various approaches such as \cite{Tam-2007}, \cite{Ruiz-2011}, \cite{Yu-2013} etc., have been explored. More details are introduced in this section.


\paragraph{Bilingual Latent Semantic Analysis}

\cite{Tam-2007} introduces the bilingual latent semantic analysis~(\textsc{blsa}) to learn the topics for both source language and target language, and apply the learned topics into language model adaptation for \textsc{smt}. \textsc{blsa} transfers the inferred topics from the source language to the parallel target language. In fact, the idea is similar to polylingual topic models~\citep{mimno-09}.

More specifically, \cite{Tam-2007} assume the aligned source document and the target document share the same document-topic distribution, thus they first learn a \textsc{lsa} model on the source language. Then they use this document-topic distribution from the source document as the document-topic distribution for the aligned target document, and then infer the topic-word distribution on the target side. The topics on the target language are not learned iteratively, thus the topics in parallel corpus can be learned very efficiently. 

In order to apply the topics for language adaptation, the word marginal distribution $p_{lsa}(w)$ for document $d$ is computed,
\begin{align}
p_{lsa}(w) = \sum_{k=1}^K p(w|k) p(k|d)
\end{align}
Then this word marginal distribution is integrated into the target background language model by minimizing the KL divergence between the adapted language model and the background language model~\citep{Kneser-1997b}:
\begin{align}
\label{eq:mdi}
p_a(w|h) \propto \Big( \frac{p_{lsa}(w)}{p_{bg}(w)} \Big) ^{\beta} \cdot p_{bg}(w|h)
\end{align}

\cite{Ruiz-2011} also apply the similar idea for language model adaptation. In stead of using \textsc{blsa}, \cite{Ruiz-2011} simply merge the aligned source and target document as one document, and perform the probabilistic latent semantic analysis. Both ideas are based on the assumption that the aligned source document and target document share the same document-topic distribution. The final adapted language model combines the topic-based language model with the general background language model, thus it is more robust in improving the results of \textsc{smt}. 

\jbgcomment{again, good to have ex}

\paragraph{Topic Projection}

\citet{Yu-2013} introduces the hidden topic markov model to improve the language model in \textsc{smt}. They build up a topic model on the source side and target side respectively, and learn a topic-specific language model based on the target side by estimating the maximum-likelihood. To smooth the sharply distributed probabilities, back-off probabilities are also considered as follows:
\begin{align}
p(w_i | w^{i-1}_{i-n+1}, k_e) = &\lambda_{w^{i-1}_{i-n+1}} p_{MLE}(w_i|w^{i-1}_{i-n+1}, k_e) \\
&+ (1- \lambda_{w^{i-1}_{i-n+1}})p(w_i|w^{i-1}_{i-n+2}, k_e)
\end{align}
where $\lambda$ is the normalization parameter, calculated by,
\begin{align}
\lambda_{w^{i-1}_{i-n+1}, k_e} = \frac{N_{1+}(w^{i-1}_{i-n-1}, k_e)}{N_{1+}(w^{i-1}_{i-n-1}, k_e) + \sum_{w_i}c(w^i_{i-n+1}, k_e)}
\end{align}
where $N_{1+}(w^{i-1}_{i-n-1}, k_e)$ is the number of words following $w^{i-1}_{i-n-1}$ in topic $k_e$, and $c(w^i_{i-n+1}, k_e)$ is the count of n-gram $w^i_{i-n+1}$ in $k_e$.

During the decoding, since no target sentence is available, they use the topic model on the source side to predict the topics for the testing data, and then project the source topic to the target topic, and then estimate the probability as following:
\begin{align}
p(e) = \sum_{k_e} p(e|k_e) p(k_e) = \sum_{k_e} p(e|k_e) \cdot \sum_{k_f} p(k_e|k_f) p (k_f)
\end{align}
where $p(k_e|k_f)$ is the topic projection probability, estimated by the co-occurrence of the source-side and the target-side topic assignment.


\section{Reordering with Topic Models}

In addition to translation models and languages models, a third important component of a phrase-based \textsc{smt} systemt is the reordering models, which learn how the order of words in the source sentences influences the order of words in the target sentences and how to make the translations in the right order. 

While a topic model helps the domain adaptation of translation models and language models, it is not obvious how it helps the reordering model, but it does. The word orders in different domains of the same language may be different, and this is where the topic models come to help. As the example shown in Table~\ref{tab:reorder-topic}~\citep{wang-14}, in \underline{economy} topic, Chinese word \begin{CJK*}{UTF8}{gbsn}比\end{CJK*} is on the left of \begin{CJK*}{UTF8}{gbsn}五\end{CJK*}; but in \underline{sports} topic, \begin{CJK*}{UTF8}{gbsn}比\end{CJK*} is on the right of \begin{CJK*}{UTF8}{gbsn}五\end{CJK*}. As a result, it is necessary to introduce domain knowledge (topics) to model such order variance. 

\begin{table}[!tp]
\begin{center}
\setlength\tabcolsep{3pt}
\begin{tabular}{c || c c} \hline
Topic & Type & Example \\ \hline \hline
\multirow{2}{*}{Economy} & Source & $\cdots$ \textbf{\begin{CJK*}{UTF8}{gbsn}比五\end{CJK*}} \begin{CJK*}{UTF8}{gbsn}月份下降\end{CJK*}$3.8\%$ $\cdots$\\
                     & Target & $\cdots$ down $3.8\%$ from May $\cdots$\\ \hline
\multirow{2}{*}{Sports} & Source & $\cdots$ \textbf{\begin{CJK*}{UTF8}{gbsn}五比\end{CJK*}}\begin{CJK*}{UTF8}{gbsn}一\end{CJK*}$3.8\%$ $\cdots$\\
                     & Target & $\cdots$ five to one $\cdots$\\ \hline
\end{tabular}
\caption{Topics influence the word orders: the Chinese words in bold are in different orders in different topics. (Example from \citet{wang-14})} 
\label{tab:reorder-topic}
\end{center}
\end{table}

This section introduces two different reordering models and how to apply topic models to improve the reordering model respectively.

\jbgcomment{I don't understand this.  Example would really help. \yhcomment{added.}}

\subsection{Lexicalized Reordering Models with Topics}

A lexicalized reordering model is to learn the orientation probabilities for a given phrase pair with respect to the previous phrase pair and the following phrase pair~\citep{Chen-2013}. The orientation $o$ typically includes three types: monotone (M), swap (S) and discontinuous (D). The M orientation means the current phrase pair is immediately to the right of the previously translated phrase in the source sentence, S orientation occurs when the current phrase is immediately to the left of the previous phrase, and all other cases all belong to D orientation. 

Formally, the reordering model is defined to estimate the corresponding probabilities $p(o|f,e)$ using recursive MAP smoothing:
\begin{align}
p(o|f,e) &= \frac{c(o,f,e) + \alpha_f p(o|f) + \alpha_e p(o|e)}{c(f,e) + \alpha_f + \alpha_e} \\
p(o|f) &= \frac{c(o,f) + \alpha_g p(o)}{c(f) + \alpha_g} \\
p(o|e) &= \frac{c(o,e) + \alpha_g p(e)}{c(e) + \alpha_g} \\
p(o) &= \frac{c(o) + \alpha_u/3}{c(\cdot) + \alpha_u}
\end{align}
where $c(o,f,e)$ is the orientation counts obtained from the word-aligned corpus;  the parameters $\alpha_f$, $\alpha_e$, $\alpha_g$ and $\alpha_u$ are learned by minimizing the perplexity of the resulting model on the held-out data.

This type of reordering model is normally referred as lexicalized since the estimated orientations depend on the words in both the previously translated phrase pair and the current phrase pair. 

\paragraph{Linear Adaptation with Topics}

\citet{Chen-2013} find that training corpus in different domains vary significantly in their reordering characteristics for particular phrase pairs, thus they introduce the linear model for reordering model adaptation. Given $N$ sub-corpora (or $N$ domain corpus), they first train a reordering model on each domain corpus, and then define the global reordering model as a linear combination of the reordering models on each domain corpus:
\begin{align}
p(o|f,e) = \sum_{i=1}^{N} \alpha_i p_i(o|f,e)
\end{align}
where $p_i(o|f,e)$ is the reordering model on sub-corpus $i$, and the weight $\alpha_i$ are learned by maximizing the probability of phrase-pair orientations in the in-domain development set:
\begin{align}
\hat{\alpha} = \texttt{argmax}_{\alpha} \sum_{o,f,e} \tilde{p}(o,f,e) \log \sum_{i=1}^N \alpha_i p_i(o|f,e)
\end{align}
where $\tilde{p}(o,f,e)$, proportional to $c(o,f,e)$, is the empirical distribution of counts in the development set. \citet{Chen-2013} propose to smooth the in-domain sample and weight instances by document frequency to further improve the mixture adaptation. They demonstrate this adaptation significantly improve the statistical machine translation system.

\subsection{Maximum Entropy Classification with Topics}

However, \citet{Chen-2013} manually divide the training data into multiple domains, instead of using automatic techniques such as topic models. \citet{wang-14} introduce topic models to the maximum entropy based phrase reordering models, proposed by \citet{Xiong-2006}. 

Under the ITG constraints~\citep{Wu-1997}, three Bracketing Transduction Grammar (BTG) rules are used to constrain the translation and reordering:
\begin{align}
A &\rightarrow [A^1, A^2]\\
A &\rightarrow <A^1, A^2>\\
A &\rightarrow f/e
\end{align}
where $A$ is a block with a pair of source and target strings; $A^1$ and $A^2$ are two consecutive blocks; the two rules are reordering rules which merge two blocks into a larger block in a straight or inverted order. Rule 3 translates a source phrase $f$ to a target phrase $e$ and generate a block $A$. These three rules are continuously used until the whole source sentence is covered, and a hierarchical segmentation tree of the source sentence is generated at the same time. 

Based on this hierarchical setting, \citet{Xiong-2006} treat the reordering problem as a classification with two labels: straight and inverted between two consecutive blocks, and build up a maximum entropy classification model as the reordering model:
\begin{align}
p(o|c(A^1, A^2)) = \frac{\exp(\sum_i \theta_i f_i(o, c(A^1, A^2)))}{\sum_{o'} \exp(\sum_i \theta_i f_i(o', c(A^1, A^2)))}
\end{align}
where $\theta_i$ are the feature weights, $c(A^1, A^2)$ indicates the attributes of $A^1$ and $A^2$, and $f_i(o, c(A^1, A^2))$ are binary features defined as,
\begin{align}
f_i(o, c(A^1,A^2)) = \begin{cases}
1, &\text{if $(o,c(A^1, A^2))$ satisfies certain condition} \\
0, &\text{else}
\end{cases}
\end{align}

This framework only uses the features extracted from the blocks
instead of the whole block (in contrast to lexicalized reordering),
thus it is flexible to reorder any blocks~\citep{Xiong-2006}.

\paragraph{Adding Topic-based Features}

Following this framework, \citet{wang-14} integrate two more types of
topic-based features into the reordering model, in additional to the
boundary word features used in \citet{Xiong-2006}. First, they choose
the topic with maximum probability in a document to be the
\textit{document topic feature} for that document. Besides, they also
use the topics of the content words that locate at the left and
rightmost positions on the source phrases as the \textit{word topic
  features} to capture topic-sensitive reordering patterns.

During the decoding process, \citet{Xiong-2006} infer the topic distributions of the test documents first and then apply this proposed topic-based reordering model as one sub-model to the log-linear model to obtain the best translation:
\begin{align}
e_\texttt{best} = \texttt{argmax}_e \Big \{ \sum_{m=1}^M \lambda_m h_m(e,f) \Big \}
\end{align}
where $h_m(e,f)$ are the  sub-models or features of the whole log-linear model, $\lambda_m$ are their weights accordingly, which are tuned on the development set.

This framework is very flexible to encode any topic-based features, and any multilingual topic models we have discussed so far can be applied to extract better topics.


\section{Beyond Domain Adaptation}

In addition to translation models, language models and reorder models, there are also other modules of \textsc{smt}, such as word alignment, where topic models have also been applied. \citet{zhao-06} build up a novel bilingual topical admixture (BiTAM) to improve the word alignment in \textsc{smt}. BiTam assumes each document pair is an admixture of topics, and the topics for each sentence pair within that document pair are sampled from the same document-topic distribution. Each topic also has a topic-specific translation table. Therefore, the sentence-level word alignment and translations are coupled by the hidden topics. This BiTam model captures the latent topical structure and generalizes word alignments and translations via topics shared across sentence pairs, thus the quality of the alignments is improved.

Besides, coherence, which ties sentences of text into a meaningfully connected structure~\citep{xiong-13}, is another important piece to \textsc{smt}.\citet{xiong-13} introduce a topic-based coherence model to improve the document translation quality. They learn the sentence topic for source documents, based on which they predict the target topic chain; they then incorporate the predicted target coherence chain into the document translation decoding process.

\jbgcomment{Need to define coherence \yhcomment{added.}}

In general, multilingual topic models obtain topics with high quality, since different languages can complement each other to reduce topic ambiguity. Many different approaches, have been explored to apply such multilingual topic models to improve different pieces of statistical machine translation. With such topic knowledge, the variations of different languages can be better captuered to make the translations more natural and coherent.

