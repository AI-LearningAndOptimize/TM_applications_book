\chapter{Multilingual Data and Machine Translation}
\label{ch:mt}

Topic models are a promising solution for automatically learning semantic coherence and discovering the global context for applications such as statistical machine translation, including improving the translation models~\citep{Eidelman-12,hu-14,zhao-06,xiao-12,xiong-13} as well as the language coherence~\citep{Bellegarda-04,wood-09}. 
While multiple languages are involved in machine translation, multilingual topic models~\citep{mimno-09,boyd-graber-10} can also be applied to extract better knowledge and apply into machine translation.

\section{Statistical Machine Translation}

Statistical machine translation casts machine translation as a probabilistic process~\citep{koehn-09}. For a parallel corpus of aligned source and target sentences $(\mathcal{F}, \mathcal{E})$, a phrase $\bar{f} \in \mathcal{F}$ is translated to a phrase $\bar{e} \in \mathcal{E}$ according to a distribution $p_w(\bar{e}|\bar{f})$.
One popular method to estimate the probability $p_w(\bar{e}|\bar{f})$ is using lexical weighting features.

Modern machine translation systems~\citep{koehn-09} use millions of examples of translations to learn translation rules in local (phrase) context, while ignoring the global (document) context. These systems work best when the training corpus has consistent global context, including genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. Early efforts focus on building separate models~\citep{foster-07} and adding features~\citep{matsoukas-09} to model domain information.  \citet{chiang-11} combine these approaches by directly optimizing genre and collection features by computing separate translation tables for each domain.

Statistical Machine Translation include three main parts: translation model, language model and reordering model. Topic models have been applied to improve each part respectively.

\section{Topic Models in Translation Models}

Topic models have been applied to improve the translation models through domain adaptation~\citep{Eidelman-12,hu-14}, word alignment~\citep{zhao-06} and translation coherence~\citep{xiao-12,xiong-13}.

\subsection{Translation Domain Adaptation with Topic Models}

Topic models are a promising solution for automatically discovering the global context---for example, domain knowledge---in machine translation corpora~\citep{Eidelman-12,hu-14}. Given the soft domain assignments from topic models, \citet{Eidelman-12} extract lexical weighting features conditioned on the topics, optimizing feature weights using the \emph{Margin Infused Relaxed Algorithm}~\citep[\textsc{mira}]{Crammer-06}.

However, \citet{Eidelman-12} ignore a wealth of information that could improve topic models and help machine translation. They reply solely on monolingual source-side models. In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a \emph{source} language to a different \emph{target} language, so existing applications of topic models~\citep{Eidelman-12} are ignoring available information on the target side that could aid domain discovery.

%\section{Multilingual Information for Domain Adaptation}

There are various ways to build up the multilingual topic models. Different languages can be connected on the word-level or the document levels. Tree-based topic models~\citep{boyd-graber-07,andrzejewski-09,Hu:Boyd-Graber:Satinoff-ur} incorporate the correlations between words in the same or different languages by encouraging words that appear together in a {\bf concept} to have similar probabilities given a topic. Polylingual topic models~\citep{mimno-09} assume that the aligned documents in different languages share the same topic distribution and each language has a unique topic distribution over its word types. \citet{hu-14} bring existing tree-based topic models and polylingual topic models together and create the polylingual tree-based topic model that incorporates both word-level correlations and document-level alignment information.

\subsection{Word Alignment with Topic Models}

In addition to applying topic models as domain adaptation, \citet{zhao-06} build up a novel bilingual topical admixture (BiTAM) to improve the word alignment in statistical machine translation. In their model, each sentence pair constitutes a mixture of hidden topics and a word pair follows a topic specific bilingual translation model. Then the BiTAM leverages the topical content of document pairs in the word alignment process to obtain better translation quality.

\subsection{Translation Coherence with Topic Models}

\citet{xiao-12} present a topic similarity model based on LDA that produces a feature that weights grammar rules based on topic compatibility. They also model the source and target side of rules and compare the target similarity during decoding by projecting the target distribution into the source space.

\citet{xiong-13} introduce a topic-based coherence model to improve the translation quality. They generate a coherence chain for each source document using topic models; predict the coherence chain for the target documents based on maximum entropy model; and incorporate the predicted target coherence chain into the document translation decoding process.

\citet{hasler-12} use the source-side topic assignments from \emph{hidden topic Markov models}~\citep[\textsc{htmm}]{gruber-07} which model documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics.  \citet{su-12} also apply \textsc{htmm} to monolingual data and apply the results to machine translation.

\section{Topic Models in Language Modeling}

Topic models capture document-level properties of language, but a critical component of machine translation systems is the language model, which provides local constraints and preferences. Domain adaptation for language models~\citep{Bellegarda-04,wood-09} is an important avenue for improving machine translation, as \citet{Bellegarda-04} points out that `` an adaptive language
model seeks to maintain an adequate representation of the current task domain under changing conditions involving
potential variations in vocabulary, syntax, content, and style''.

\citet{wood-09} present a hierarchical Pitman-Yor process language model, where the bottom layer include language models in different domains and the top layer couples multiple language models to share statistical length.

\section{Reordering with Topic Models}

Topic models have also been applied to improve the reordering model, another important component in statistical machine translation.
\citet{wang-14} predict orders for neighboring blocks by capturing topic-sensitive reordering patterns: they learn the reordering examples from bilingual data based on the document-level and word-level topic information from topic models, and then train a topic-based reordering model that based on a maximum entropy classifier. 


