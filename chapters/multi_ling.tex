\chapter{Multilingual Data and Machine Translation}
\label{ch:mt}

Modern machine translation systems~\citep{koehn-09} use millions of examples of translations to learn translation rules in local (phrase) context, while ignoring the global (document) context. These systems work best when the training corpus has consistent global context, including genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit \textbf{domain adaptation}. 

Topic models are a promising solution for automatically learning semantic coherence and discovering the domain knowledge as global context for applications such as statistical machine translation, including improving the translation models~\citep{Eidelman-12,hu-14,zhao-06,xiao-12,xiong-13} as well as the language coherence~\citep{Bellegarda-04,wood-09}. 
While multiple languages are involved in machine translation, multilingual topic models~\citep{mimno-09,boyd-graber-10} can also be applied to extract better knowledge and apply into machine translation.

\section{Statistical Machine Translation}

Statistical machine translation casts machine translation as a probabilistic process~\citep{koehn-09}. Here we briefly introduce the standard phrase translation model introduced in~\citep{koehn-03}. Given the source sentence $\mathbf{f}$, the best translation in target language $\mathbf{e}_\texttt{best}$ is modeled as,
\begin{equation}
\mathbf{e}_\texttt{best} = \textbf{argmax}_\mathbf{e} p(\mathbf{e}|\mathbf{f}) = \textbf{argmax}_\mathbf{e} p(\mathbf{f}|\mathbf{e}) p (\mathbf{e})
\end{equation}
which is split to a \textit{translation model} $p(\mathbf{f}|\mathbf{e})$ and a \textit{language model} $p (\mathbf{e})$. 

The source sentence $\mathbf{f}$ is segmented into multiple source phrases $\bar{f}_i$ during the decoding, which are translated to a set of target phrases $\bar{e}_i$. Thus the translation probability $p(\mathbf{f}|\mathbf{e})$ can be further decomposed to the phrase translation probability $p(\bar{f}_i | \bar{e}_i)$. Besides, the target phrases may need to be \textit{reordered} to get the best translation result, and this part is captured by a relative distortion probability distribution $d(a_i - b_{i-1})$, where $a_i$ denotes the start position of the source phrase that was translated to the $i$th target phrase, and $b_{i-1}$ denotes the end position of the source phrase translated into the $(i-1)$th target phrase. As a result, the translation model can be decomposed as,
\begin{equation}
p(\mathbf{f}|\mathbf{e}) = \prod_{i} p(\bar{f}_i | \bar{e}_i) d(a_i - b_{i-1})
\end{equation}

In phrase-based \textsc{smt}, the phrase probability $p(\bar{f}_i | \bar{e}_i)$ can be further estimated by combining lexical translation probabilities of words contained in that phrase~\citep{koehn-03}, which is normally referred as \textit{lexical weighting}. Lexical conditional probabilities $p_w(f|e)$ are maximum likelihood estimates from relative lexical frequencies,
\begin{equation}
\label{eq:lexical_prob}
p_w(f|e) = \textstyle \slfrac{c(f, e)}{\sum_f{c(f, e)}}
\end{equation}
where $c(f, e)$ is the count of observing lexical pair $(f, e)$ in the training dataset. Given a word alignment $a$, the lexical weight for this phrase pair $p_w(\bar{f} | \bar{e}; a)$ is the normalized product of lexical probabilities of the aligned word pairs within that phrase pair:
\begin{equation}
\label{eq:phrase_prob}
p_w(\bar{f} | \bar{e}; a) = \prod^{n}_{i=1} \frac{1}{\{|j | (i, j) \in a\}|} \sum_{\forall (i,j) \in a} p_w(f_i | e_j)
\end{equation}
where $i$ and $j$ are the word positions in target phrase $\bar{e}$ and source phrase $\bar{f}$ respectively.

\paragraph{\textsc{smt} with Domain Adaptation}

A \textsc{smt} system is usually trained on documents with the same genre (e.g., sports, business) from a similar style (e.g., newswire, blog-posts).  These are called \emph{domains}.  Translations within one domain are better than translations across domains since they vary dramatically in their word choices and style.  A correct translation in one domain may be inappropriate in another domain.  For example, ``\begin{CJK*}{UTF8}{gbsn}潜水\end{CJK*}'' in a newspaper usually means ``underwater diving''.  On social media, it means a non-contributing ``lurker''.

To Train such \textsc{smt} systems with domain adaptation, early efforts focus on building separate models~\citep{foster-07} and adding features~\citep{matsoukas-09} to model domain information.  \citet{chiang-11} combine these approaches by directly optimizing genre and collection features by computing separate translation tables for each domain.

However, these approaches treat domains as hand-labeled, constant, and known \textit{a priori}.  This setup is at best expensive and at worst infeasible for large data.  Topic models provide a solution where domains can be automatically induced from raw data: treat each topic as a domain. \footnote{Henceforth we will use the term ``topic'' and ``domain'' interchangeably: ``topic'' to refer to the concept in topic models and ``domain'' to refer to \textsc{smt} corpora.}
Such global context from topic models can be applied to improve each component of the statistical machine translation systems. Next we introduce how to apply such domain knowledge from topic models to translation models, language models and reordering models respectively.

\section{Topic Models in Translation Models}

%Topic models have been applied to improve the translation models through domain adaptation~\citep{Eidelman-12,hu-14}, word alignment~\citep{zhao-06} and translation coherence~\citep{xiao-12,xiong-13}.

Topic models are a promising solution for automatically discovering the global context---for example, domain knowledge---in machine translation corpora~\citep{Eidelman-12,hu-14}. Given the soft domain assignments from topic models, \citet{Eidelman-12} extract lexical weighting features conditioned on the topics, optimizing feature weights using the \emph{Margin Infused Relaxed Algorithm}~\citep[\textsc{mira}]{Crammer-06}.

\subsection{Translation Domain Adaptation with Topic Models}

Topic models take the number of topics $K$ and a collection of documents as input, where each document is a bag of words. They output two distributions: a distribution over topics for each document $d$; and a distribution over words for each topic. If each topic defines a \textsc{smt} domain, the document's topic distribution is a soft domain assignment for that document.

Given the soft domain assignments, \citet{Eidelman-12} extract lexical weighting features conditioned on the topics, optimizing feature weights using the \emph{Margin Infused Relaxed Algorithm}~\cite[\textsc{mira}]{Crammer:2006}.  The topics come from source documents \emph{only} and create topic-specific lexical weights from the per-document topic distribution $p(k|d)$, which is used to smooth the expected count $\hat{c}_{k}(f,e)$ of a word translation pair under topic $k$,
\begin{align}
\textstyle \hat{c}_{k}(f,e) = \sum_{d}{p(k|d)c_d(f,e)},
\end{align}
where $c_d(\bullet)$ is the number of occurrences of the word pair in document $d$.  The lexical probability conditioned on topic $k$ is the unsmoothed probability estimate of those expected counts
\begin{align}
\label{eq:lexical_prob_k}
\textstyle p_w(f|e;k) = \hat{c}_{k}(f,e) / \sum_f{\hat{c}_{k}(f,e)},
\end{align}
from which we can compute the lexical weight of this phrase pair
$p_w(\bar{f}|\bar{e};a, k)$ given a word alignment $a$\citep{koehn-03}:
\begin{align}
\label{eq:phrase_prob_k}
p_w(\bar{f} | \bar{e};a, k) = \prod^{n}_{i=1} \frac{1}{\{|j | (i, j) \in a\}|} \sum_{\forall (i,j) \in a} p_w(f_i | e_j; k)
\end{align}
where $i$ and $j$ are the word positions in target phrase $\bar{e}$ and source phrase $\bar{f}$ respectively. Comparing Equation~\ref{eq:lexical_prob_k} against Equation~\ref{eq:lexical_prob}, and Equation~\ref{eq:phrase_prob_k} against Equation~\ref{eq:phrase_prob}, we can clearly see how the topics are encoded as soft domains and how these topics influence the probability. 

For a test document $d$, the document topic distribution $p(k | d)$ is inferred based on the topics learned from training data. The lexical weight feature of a phrase pair $(\bar{f}, \bar{e})$ is,
\begin{align}
\textstyle f_{k}(\bar{f}|\bar{e})=-\log\left\{{p_{w}(\bar{f}|\bar{e};k)\cdot p(k|d)}\right\},
\end{align}
a combination of the topic dependent lexical weight and the topic distribution of the document, from which we extract the phrase.

In fact, \citet{Eidelman-12} introduces the two direction topic-adapted probabilities instead of a single direction: $p_w(\bar{f} | \bar{e};a, k)$ and $p_w(\bar{e} | \bar{f};a, k)$. This is equivalent to introduce $2K$ new word translation tables, thus it results in introducing $2K$ lexical weight features $f_{k}(\bar{f}|\bar{e})$ and $f_{k}(\bar{e}|\bar{f})$. Given the topic-adapted features, \citet{Eidelman-12} compute the resulting model score by combining these adapted features in a linear model with other standard \textsc{smt} features and optimizing the weights:
\begin{align}
\explain{standard features}{\sum_p \lambda_p f_p(\bar{f},\bar{e})} + \explain{adapted features}{\sum_k \lambda_k f_k(\bar{f} | \bar{e})} + \explain{adapted features}{\sum_k \lambda_k f_k(\bar{e} | \bar{f})}
\end{align}
These adapted features allow us to bias the translations according to the topics. For example, if topic $k$ is dominant in a test document, the feature $f_k(\bar{f} | \bar{e})$ will be large, which may bias the decoder to a translation that has small value of the standard feature $f_p(\bar{f}, \bar{e})$. In addition, combining the adapted features with the standard features makes this model more flexible. For a test document with less clear topics, the topic distribution will tend toward being fairly uniform. In this case, the topic features will contribute less to the translation results and the standard features will dominate the translation results.

Conceptually, this approach is just reweighting examples.  The probability of a topic given a document is never zero.  Every translation observed in the training set will contribute to $p_{k}(e|f)$; many of the expected counts, however, will be less than one.  This obviates the explicit smoothing used in other domain adaptation systems~\citep{chiang-11}. 

However, \citet{Eidelman-12} ignore a wealth of information that could improve topic models and help machine translation. They rely solely on monolingual source-side models. In contrast, machine translation uses inherently multilingual data: an \textsc{smt} system must translate a phrase or sentence from a \emph{source} language to a different \emph{target} language, so existing applications of topic models~\citep{Eidelman-12} are ignoring available information on the target side that could aid domain discovery. This can be improved by introduction multilingual topic models for machine translation, and the details are introduced in the next section.

\subsection{Multilingual Information for Domain Adaptation}

As we mentioned in the previous section, \citet{Eidelman-12} only use monolingual data from the source language for topic modeling, ignoring all target-language data and available lexical semantic resources between source and target languages. In fact, different languages complement each other to reduce ambiguity.  For example, ``\begin{CJK*}{UTF8}{gbsn}木马\end{CJK*}'' in a Chinese document can be either ``hobbyhorse'' in a children's topic, or ``Trojan virus'' in a technology topic.  A short Chinese context obscures the true topic. However, these terms are unambiguous in English, revealing the true topic. From this example, we can see that that topic models on multilingual corpus do improve the quality of the extracted topics.

There are various ways to build up the multilingual topic models. Different languages can be connected on the word-level or the document levels. 
On one hand, \emph{lexical information}, such as orthographic similarity~\citep{boyd-graber-09} and multilingual dictionaries~\citep{boyd-graber-10}, can be very helpful to induce better topics from multilingual corpora. For instance, tree-based topic models~\citep{boyd-graber-07,andrzejewski-09,Hu:Boyd-Graber:Satinoff-ur} incorporate the correlations between words in the same or different languages by encouraging words that appear together in a {\bf concept} to have similar probabilities given a topic. 
On the other hand, topic models can take advantage of document \emph{alignment information} and infer more robust topics from the aligned dataset. For example, polylingual topic models~\citep{mimno-09} assume that the aligned documents in different languages share the same topic distribution and each language has a unique topic distribution over its word types. 

These two approaches are not mutually exclusive, however; they reveal different connections across languages. \citet{hu-14} bring existing tree-based topic models~(\tlda{}) and polylingual topic models~(\plda{}) together and create the polylingual tree-based topic model~(\ptlda{}) that incorporates both word-level correlations and document-level alignment information. They further apply the proposed polylingual tree-based topic model in the \textsc{smt} framework from \citet{Eidelman-12} to improve the machine translation results.





\paragraph{\bf Word-level Correlations}

Tree-based topic models incorporate the positive correlations between words by encouraging words that appear together in a {\bf concept}~(referred as ``positive correlations'' in \citet{andrzejewski-09,Hu:Boyd-Graber:Satinoff-ur}) to have similar probabilities given a topic. These concepts can come from WordNet~\citep{boyd-graber-10}, domain experts~\citep{andrzejewski-09}, or user constrains~\citep{Hu:Boyd-Graber:Satinoff-ur}. When we gather concepts from bilingual resources, these concepts can connect different languages.  For example, if a bilingual dictionary defines ``\begin{CJK*}{UTF8}{gbsn}电脑\end{CJK*}'' as ``computer'', we combine these words in a concept.

These concepts (positive correlations) are organized into a {\bf prior tree} structure. As Figure~\ref{fig:prior_trees} shows, words in the same concept share a common parent node, and then that concept becomes one of many children of the root node.  Words that are not in any concept---{\bf uncorrelated words}---are directly connected to the root node.

When this tree serves as a prior for topic models, words in the same concept are positively correlated in topics. 
For example, if ``\begin{CJK*}{UTF8}{gbsn}电脑\end{CJK*}'' has high probability in a topic, so will ``computer'', since they share the same parent node. With the tree priors, each topic is no longer a distribution over word types; instead, it is a distribution over paths, and each path is
associated with a word type.  The same word could appear in multiple paths, and each path represents a unique sense of this word.

\paragraph{\bf Document-level Alignments}

Lexical resources connect languages and help guide the topics. However, these resources are sometimes brittle and may not cover the whole vocabulary.  Aligned document pairs provide a more corpus-specific, flexible association across languages.

\citet{Landauer-1990} connect documents in different languages by projecting both documents to a shared latent semantic indexing space. Similarly, 
polylingual topic models~\citep{mimno-09} assume that the aligned documents in different languages share the same topic distribution and each language has a unique topic distribution over its word types.  This connection between languages is flexible: instead of requiring the exact matching on words and sentences, only a coarse document alignment is necessary, as long as the documents discuss the same topics.

\paragraph{\bf Combine Words and Documents}

\citet{hu-14} introduce polylingual tree-based topic models (\ptlda{}), which connect information across different languages by incorporating both
word correlation (as in \tlda{}) and document alignment information (as in \plda{}). 

To build up the prior tree structure, \citet{hu-14} consider two resources to build trees that correlate words across languages. The first is multilingual dictionaries~(\textit{dict}), which match words with the same meaning in different languages together.  These relations between words are used as the concepts~\citep{Bhattacharya-2006} in the prior tree (Figure~\ref{fig:prior_trees}). In addition, they extract the word alignments from aligned sentences in a parallel corpus.  The word pairs define concepts for the prior tree (\textit{align}). 

\begin{figure}
\centering
%\includegraphics[width=0.9\linewidth]{figures/correlations_tree-crop.pdf}
\vspace{-3mm}
\caption[Constructing prior tree from a bilingual dictionary]{An example of constructing a prior tree from a
  bilingual dictionary: word pairs with the same meaning but in
  different languages are concepts; we create a common parent node to
  group words in a concept, and then connect to the root;
  uncorrelated words are connected to the root directly.  Each topic
  uses this tree structure as a prior. }
\label{fig:prior_trees}
\end{figure}

Given the prior tree structure, the proposed polylingual tree-based topic models introduce an additional step of selecting a concept in a topic responsible for generating each word token. This is represented by a path $y_{d,n}$ through the topic's tree. This is the same as \tlda{}, but different from \abr{lda}, where each word is associated with a topic. 

\begin{algorithm}[t!]
\begin{footnotesize}
\caption{\textsc{\small{Generative Process for \ptlda{}}}}
\label{alg:ptlda}
\begin{algorithmic}[1]
  \FOR{topic $k \in {1, \cdots, K}$}
  \FOR{each internal node $n_i$}
  \STATE draw a distribution $\pi_{ki} \sim \text{Dir}(\beta_i)$
  \ENDFOR
  \ENDFOR
  \FOR{document set $d \in {1, \cdots, D}$}
  \STATE draw a distribution $\theta_d \sim \text{Dir}(\alpha)$
  \FOR{each word in documents $d$}
  \STATE choose a topic $z_{dn} \sim \text{Mult}(\theta_d)$
  \STATE sample a path $y_{dn}$ with probability $\prod_{(i,j) \in y_{dn}} \pi_{z_{dn}, i, j}$
  \STATE $y_{dn}$ leads to word $w_{dn}$ in language $l_{dn}$
  \STATE append token $w_{dn}$ to document $d_{l_{dn}}$
  \ENDFOR
  \ENDFOR
\end{algorithmic}
\end{footnotesize}
\end{algorithm}


The probability of a path in a topic depends on the transition probabilities in a topic.  Each concept $i$ in topic $k$ has a distribution over its children nodes is governed by a Dirichlet prior: $\pi_{k,i} \sim \text{Dir}(\beta_{i})$.  Each path ends in a word (i.e., a leaf node) and the probability of a path is the product of all of the transitions between topics it traverses. Topics have correlations over words because the Dirichlet parameters can encode positive or negative correlations~\citep{andrzejewski-09}.

With these correlated in topics in hand, the generation of documents is very similar to \textsc{lda}.  For every document $d$, we first sample a distribution over topics $\theta_d$ from a Dirichlet prior $\text{Dir}(\alpha)$.  For every token in the documents, we first sample a topic $z_{dn}$ from the multinomial distribution $\theta_d$, and then sample a path $y_{dn}$ along the tree according to the transition distributions specified by topic $z_{dn}$.  Because every path $y_{dn}$ leads to a word $w_{dn}$ in language $l_{dn}$, we append the sampled word $w_{dn}$ to document $d_{l_{dn}}$.  Aligned documents have words in both languages; monolingual documents only have words in a single language. The full generative process is shown in Algorithm~\ref{alg:ptlda}.


If a flat symmetric Dirichlet prior is used instead of the tree prior, \plda{} is recovered; and if all documents are monolingual (i.e., with distinct distributions over topics $\theta$), \tlda{} is recovered. \ptlda{} connects different languages on both the word level (using the word correlations) and the document level (using the document alignments).

\section{Topic Models in Language Modeling}

Topic models capture document-level properties of language, but a critical component of machine translation systems is the language model, which provides local constraints and preferences. Domain adaptation for language models~\citep{Bellegarda-04,wood-09} is an important avenue for improving machine translation, as \citet{Bellegarda-04} points out that `` an adaptive language model seeks to maintain an adequate representation of the current task domain under changing conditions involving potential variations in vocabulary, syntax, content, and style''.

A language model describes the probability of a word $w$ occurring given the previous context words, which is also mentioned as the history $h$. Language model adaptation uses extra knowledge to adjust this probability $p(w|h)$ to reflect the content change. Topics from topic models can be one of the resources to provide such knowledge for language model adaptation.

Early work such as \citet{Clarkson-1997,Seymore-1997,Kneser-1997,Iyer-1999} focus on partitioning the training data to multiple topic-specific subsets and building up language models for each subset. Then the topic-specific language models $p_k(w|h)$ are linearly combined with a general language model $p_g(w|h)$ built from all training data as Equation~\ref{eq:linear_lm}. The weights $\lambda_k$ can be tuned based on the topics of the test documents. 
\begin{align}
\label{eq:linear_lm}
p_\textrm{adapted}(w|h) = \sum_k \lambda_k p_k(w|h) + \lambda_g p_g(w|h)
\end{align} 

\citet{Seymore-1998} further identify the most qualified topic for each word in the vocabulary and choose a topic-specific language model or the general language model. The intuition is that the general language model provides the most reliable estimation for general words, and the topic language model estimates the probability more accurately for topic words. As a result, they split the vocabulary words into three groups: the general subset, on-topic subset and off-topic subsets. For general subset and off-topic subset, the general language model provides the word probability; and the topic-specific language model provides the word probability for the on-topic subset. 

While these early attempts introduce the topic mixture to improve the language models, these topic-specific language models are just like the traditional n-gram models, which can model the limited history. Besides, these models assume each document or history belongs to exactly one topic cluster. 

To fix these problems, \citet{Bellegarda-1997,Coccaro-1998} introduce the \emph{Latent Semantic Analysis}~\citep[\textsc{lsa}]{deerwester-90} to learn large-span language model. However, \textsc{lsa} is based on Singular Value Decomposition techniques. Thus \citet{Gildea-1999} further introduces the probabilistic interpretation---the probabilistic latent semantic indexing~\citep[\textsc{plsi}]{hoffman-99}---to language models. \citet{Gildea-1999} decomposes the language model based on topics,
\begin{align}
\label{eq:plsi_lm}
p(w|h) = \sum_k p(w|k) p(k|h)
\end{align}
where the topics are learned from the training corpus by optimizing the log probability,
\begin{align}
l(\theta; N) = \sum_w \sum_d n(w,d) \log \sum_k p(w|k) p(k|d)
\end{align}
where $d$ is the training documents, and $n(w,d)$ is the word frequency of $w$ in document $d$. $p(w|k)$ and $p(k|d)$ are learned through the EM algorithm. For test documents, they fix $p(w|k)$ to estimate $p(t|h)$ and then compute $p(w|h)$ using Equation~\ref{eq:plsi_lm}. However, the topic models do not take advantage of short-range structure. To combine the topic models with the general n-gram language model, \citet{Gildea-1999} assume that the history $h$ and the n-gram context are independent conditioned on $w_i$ as the following,
\begin{align}
p(w_i|h,w_{i-n+1}^{i-1}) \approx \frac{p(w|h)p(w|w_{i-n+1}^{i-1})}{p(w_i)}
\end{align}
Even though this assumption is not valid in general, it works better than the linear scale or log-scale combination according to \citet{Gildea-1999}.

\cite{Tam-2007} and \cite{Ruiz-2011}



\citet{Yu-2013} introduces the hidden topic markov model to improve the language model in \textsc{smt}. They build up a topic model on the source side and target side respectively, and learn a topic-specific language model based on the target side by estimating the maximum-likelihood. To smooth the sharply distributed probabilities, back-off probabilities are also considered as follows:
\begin{align}
p(w_i | w^{i-1}_{i-n+1}, k_e) = &\lambda_{w^{i-1}_{i-n+1}} p_{MLE}(w_i|w^{i-1}_{i-n+1}, k_e) \\
&+ (1- \lambda_{w^{i-1}_{i-n+1}})p(w_i|w^{i-1}_{i-n+2}, k_e)
\end{align}
where $\lambda$ is the normalization parameter, calculated by,
\begin{align}
\lambda_{w^{i-1}_{i-n+1}, k_e} = \frac{N_{1+}(w^{i-1}_{i-n-1}, k_e)}{N_{1+}(w^{i-1}_{i-n-1}, k_e) + \sum_{w_i}c(w^i_{i-n+1}, k_e)}
\end{align}
where $N_{1+}(w^{i-1}_{i-n-1}, k_e)$ is the number of words following $w^{i-1}_{i-n-1}$ in topic $k_e$, and $c(w^i_{i-n+1}, k_e)$ is the count of n-gram $w^i_{i-n+1}$ in $k_e$.

During the decoding, since no target sentence is available, they use the topic model on the source side to predict the topics for the testing data, and then project the source topic to the target topic, and then estimate the probability as following:
\begin{align}
p(e) = \sum_{k_e} p(e|k_e) p(k_e) = \sum_{k_e} p(e|k_e) \cdot \sum_{k_f} p(k_e|k_f) p (k_f)
\end{align}
where $p(k_e|k_f)$ is the topic projection probability, estimated by the co-occurrence of the source-side and the target-side topic assignment.


\section{Reordering with Topic Models}

Topic models have also been applied to improve the reordering model, another important component in statistical machine translation.
\citet{wang-14} predict orders for neighboring blocks by capturing topic-sensitive reordering patterns: they learn the reordering examples from bilingual data based on the document-level and word-level topic information from topic models, and then train a topic-based reordering model that based on a maximum entropy classifier. 

\citet{Chen-2013}

\section{Beyond Domain Adaptation}

\subsection{Word Alignment with Topic Models}

In addition to applying topic models as domain adaptation, \citet{zhao-06} build up a novel bilingual topical admixture (BiTAM) to improve the word alignment in statistical machine translation. In their model, each sentence pair constitutes a mixture of hidden topics and a word pair follows a topic specific bilingual translation model. Then the BiTAM leverages the topical content of document pairs in the word alignment process to obtain better translation quality.

\subsection{Translation Coherence with Topic Models}

\citet{xiao-12} present a topic similarity model based on LDA that produces a feature that weights grammar rules based on topic compatibility. They also model the source and target side of rules and compare the target similarity during decoding by projecting the target distribution into the source space.

\citet{xiong-13} introduce a topic-based coherence model to improve the translation quality. They generate a coherence chain for each source document using topic models; predict the coherence chain for the target documents based on maximum entropy model; and incorporate the predicted target coherence chain into the document translation decoding process.

\citet{hasler-12} use the source-side topic assignments from \emph{hidden topic Markov models}~\citep[\textsc{htmm}]{gruber-07} which model documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics.  \citet{su-12} also apply \textsc{htmm} to monolingual data and apply the results to machine translation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}

\chapter{Multilingual Data and Machine Translation --- Structure}
\label{ch:mt}

Topic models are a promising solution for automatically learning semantic coherence and discovering the global context for applications such as statistical machine translation, including improving the translation models~\citep{Eidelman-12,hu-14,zhao-06,xiao-12,xiong-13} as well as the language coherence~\citep{Bellegarda-04,wood-09}. 
While multiple languages are involved in machine translation, multilingual topic models~\citep{mimno-09,boyd-graber-10} can also be applied to extract better knowledge and apply into machine translation.

\section{Statistical Machine Translation}

Statistical machine translation casts machine translation as a probabilistic process~\citep{koehn-09}. For a parallel corpus of aligned source and target sentences $(\mathcal{F}, \mathcal{E})$, a phrase $\bar{f} \in \mathcal{F}$ is translated to a phrase $\bar{e} \in \mathcal{E}$ according to a distribution $p_w(\bar{e}|\bar{f})$.
One popular method to estimate the probability $p_w(\bar{e}|\bar{f})$ is using lexical weighting features.

Modern machine translation systems~\citep{koehn-09} use millions of examples of translations to learn translation rules in local (phrase) context, while ignoring the global (document) context. These systems work best when the training corpus has consistent global context, including genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. Early efforts focus on building separate models~\citep{foster-07} and adding features~\citep{matsoukas-09} to model domain information.  \citet{chiang-11} combine these approaches by directly optimizing genre and collection features by computing separate translation tables for each domain.

Statistical Machine Translation include three main parts: translation model, language model and reordering model. Topic models have been applied to improve each part respectively.

\section{Topic Models in Translation Models}

Topic models have been applied to improve the translation models through domain adaptation~\citep{Eidelman-12,hu-14}, word alignment~\citep{zhao-06} and translation coherence~\citep{xiao-12,xiong-13}.

\subsection{Translation Domain Adaptation with Topic Models}

Topic models are a promising solution for automatically discovering the global context---for example, domain knowledge---in machine translation corpora~\citep{Eidelman-12,hu-14}. Given the soft domain assignments from topic models, \citet{Eidelman-12} extract lexical weighting features conditioned on the topics, optimizing feature weights using the \emph{Margin Infused Relaxed Algorithm}~\citep[\textsc{mira}]{Crammer-06}.

However, \citet{Eidelman-12} ignore a wealth of information that could improve topic models and help machine translation. They reply solely on monolingual source-side models. In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a \emph{source} language to a different \emph{target} language, so existing applications of topic models~\citep{Eidelman-12} are ignoring available information on the target side that could aid domain discovery.

%\section{Multilingual Information for Domain Adaptation}

There are various ways to build up the multilingual topic models. Different languages can be connected on the word-level or the document levels. Tree-based topic models~\citep{boyd-graber-07,andrzejewski-09,Hu:Boyd-Graber:Satinoff-ur} incorporate the correlations between words in the same or different languages by encouraging words that appear together in a {\bf concept} to have similar probabilities given a topic. Polylingual topic models~\citep{mimno-09} assume that the aligned documents in different languages share the same topic distribution and each language has a unique topic distribution over its word types. \citet{hu-14} bring existing tree-based topic models and polylingual topic models together and create the polylingual tree-based topic model that incorporates both word-level correlations and document-level alignment information.

\subsection{Word Alignment with Topic Models}

In addition to applying topic models as domain adaptation, \citet{zhao-06} build up a novel bilingual topical admixture (BiTAM) to improve the word alignment in statistical machine translation. In their model, each sentence pair constitutes a mixture of hidden topics and a word pair follows a topic specific bilingual translation model. Then the BiTAM leverages the topical content of document pairs in the word alignment process to obtain better translation quality.

\subsection{Translation Coherence with Topic Models}

\citet{xiao-12} present a topic similarity model based on LDA that produces a feature that weights grammar rules based on topic compatibility. They also model the source and target side of rules and compare the target similarity during decoding by projecting the target distribution into the source space.

\citet{xiong-13} introduce a topic-based coherence model to improve the translation quality. They generate a coherence chain for each source document using topic models; predict the coherence chain for the target documents based on maximum entropy model; and incorporate the predicted target coherence chain into the document translation decoding process.

\citet{hasler-12} use the source-side topic assignments from \emph{hidden topic Markov models}~\citep[\textsc{htmm}]{gruber-07} which model documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics.  \citet{su-12} also apply \textsc{htmm} to monolingual data and apply the results to machine translation.

\section{Topic Models in Language Modeling}

Topic models capture document-level properties of language, but a critical component of machine translation systems is the language model, which provides local constraints and preferences. Domain adaptation for language models~\citep{Bellegarda-04,wood-09} is an important avenue for improving machine translation, as \citet{Bellegarda-04} points out that `` an adaptive language model seeks to maintain an adequate representation of the current task domain under changing conditions involving potential variations in vocabulary, syntax, content, and style''.

\citet{wood-09} present a hierarchical Pitman-Yor process language model, where the bottom layer include language models in different domains and the top layer couples multiple language models to share statistical length.

\section{Reordering with Topic Models}

Topic models have also been applied to improve the reordering model, another important component in statistical machine translation.
\citet{wang-14} predict orders for neighboring blocks by capturing topic-sensitive reordering patterns: they learn the reordering examples from bilingual data based on the document-level and word-level topic information from topic models, and then train a topic-based reordering model that based on a maximum entropy classifier. 

\end{comment}