

\chapter{Building a Topic Model}
\label{ch:building}

Previous chapters have focused on \emph{existing} models.  This
chapter focuses on how a researcher can create, implement, and
validate a new model.  We have described
models that researchers have created to capture particular nuances or
processes that exist in the world, but omit blueprints
for research who want to extend an existing model or create their own.

It is impossible to cover all of the details of research in topic
models or machine learning generally, but this chapter introduces some
of the common techniques for creating new models.  We will focus on a
running example, creating a new model for predicting the
ideology of a political speaker.

\section{Designing a Model}

The first step in creating a new model is to define what is
important.  For example, previous chapters have focused on measuring
innovation, cross-language connections, and sentiment.  These are
high-level concepts that we want do discover from text.  We believe
these properties exist in the world, but we want the variables of our models to represent
these concepts.

In a topic model, incorporating a new concept into the model usually
involves adding a new random variable to the model.  This is where
intuitions and domain knowledge come to the forefront.  Because the
generative process attempts to model the real world, a new model must
balance several components that are often in tension: fidelity,
performance, tractability, and interpretability.

\paragraph{Fidelity}

A good model should reflect the world.  If political scientists
believe that a politician's ideology is a property that changes how they speak, then the model should place a
speaker's ideology upstream of the language politicians produce
(upstream vs. downstream models are discussed in Chapter~\ref{ch:css}).

Modeling reality is a good idea. But just as building a scale model of a building requires compromises in terms of materials and level of detail, building a statistical model requires unreal assumptions.
If a generative model exactly
matches the process that produces data, we can prove that it will
converge to the correct answer ~\cite{}.
But humans and text are
not Dirichlet and multinomial distributions; all models are going to
be an approximation.

In addition to determining \emph{where} to model a feature in a
generative model, it is also important to decide \emph{how} to model
it.  Is it a continuous value, a binary value, a member of a discrete
set, or something else?  Often, we have multiple pre-defined notions of how to
represent a quantity of interest.
Sentiment is sometimes represented as a continuous positive or negative value, while review corpora include discrete, positive star ratings.
Political ideology could be represented by membership in one of a fixed number of
parties, but political scientists more often assign a continuous value to a
politician's ideology.

\paragraph{Performance}

Greater fidelity to our perceptions of how the world works does not always imply that models will be more  useful.
We sometimes have to trade off fidelity with performance.
Recent work seems to agree that \emph{downstream} models work better even though it is
less realistic.\footnote{One might argue that it is more realistic for
  a \emph{listener} who must interpret speech to assign an ideology
  after it has been heard, but this is no longer consistent with how
  the text was \emph{generated}.}  %% CITE?

It is nearly impossible to know \textit{a priori} whether a
model will work well for a particular task given just the model.
Knowing what will work best is often a trial-and-error
process.
However, it is often possible to draw parallels from similar models.
Upstream models allow metadata variables to better predict which topics
will occur in a context, but they do not necessarily encourage the model to 
learn different topics than a model without metadata.
Downstream models must align topics to best predict a metadata variable, 
and therefore tend to find different (but not necessarily better) topics.
Supervised topic models have therefore worked well for predicting sentiment as a downstream variable, so it might be reasonable to assume
that a downstream model would work well for ideology as well.

\paragraph{Tractability}

Now that we know what we want to model, some approaches to model the
variable could be easier than others.  Political ideology is often
thought of as a \emph{spectrum} rather than a label: some politicians
are more centrist than others even though they might have the same
party label.

However, discrete labels are appealing from a modeling perspective.
Combinations of Dirichlet-Multinomial distributions (Chapter~\ref{})
are easy to to combine.  Given that basic topic models are built from
Dirichlets and Multinomials, it is easier to add an additional
Dirichlet-Multinomial than to add a continuous random variable to the
model.  For example, the Topic-Aspect model~\citep{} is far simpler to
implement than Supervised Topic Models~\citep{}.

Dirichlets and Multinomials play well together because they are
conjugate~\citep{}, while Gaussian distributions add additional
difficulty.  However, Gaussian distributions are more convenient than
other distributions.  For example, spherical distributions~\citep{} are thought
to better model continuous embeddings of words than Gaussian
distributions but come at the cost of a less tractable model.

Even worse are combinatorial probability distributions.  For example,
in Chapter~\ref{} we discussed how to learn mappings across
languages.  \citet{} used a combinatorial distribution to learn the
mapping from one language to another.  It is very complicated but
does not model languages as well as far simpler approaches~\citep{}.

Our listing of ever more complicated distributions should not be taken
as an admonition against using them: sometimes complicated models are
necessary.  However, one should not choose a complicated model just
because it is complicated (an alluring temptation to young graduate
students who want to show off their machine learning chops).  It is
best to try the simplest possible model that could work; even if it
fails, this model can serve as a useful baseline.

Unlike many of the other dimensions when building a model, complexity
is often sought and fetishized without improving the other dimensions
(perverse incentives from publications often play a role in this).
Thus, it is particularly important to guard against unnecessarily
complicating a model.

\paragraph{Interpretability}

As we describe in Chapter~\ref{}, interpretability is a measure of how
easy it is for a human to understand the results of a model.  Often,
the interpretability of a model is an afterthought.  That's if it is thought of
at all; many papers neglect to inspect the learned parameters of a
model, focusing instead on the performance.

While always a problem, wIth the increased prominence of deep learning (discussed more in
Chapter~\ref{}), this is particularly worrying.  Deep learning has a
reputation for inscrutable parameters but state-of-the-art
performance.  One of the strengths of probabilistic models is their
interpretability and grounded generative processes.  Thus, researchers who
choose probabilistic models such as topic models should not ignore the
interpretability of their models.

While Chapter~\ref{} discusses ways of rigorously evaluating the
interpretability of models, even a simple once-over of model
parameters by the researcher can reveal whether a learned model
``makes sense'' or not.  Inspecting the model can help buttress
whether the design of the model (``fidelity'', above) was able to
capture the intuitions of the modeler.

However, there are often tradeoffs with interpretability (as with the
choice of a deep learning model).  \citet{} found that the Correlated
Topic Model~\citep{} had markedly higher held-out likelihood at the
cost of interpretability.

\section{Implementing the Model}

So you have a new model in hand.  This is usually described as a
generative process (Chapter~\ref{}): a sequence of probabilistic steps
that tells a story of how your data came to be.

Implementing a model requires writing down this model in a form that a
computer can understand and then using \emph{probabilistic inference}
work backward from data to discover the configuration of the latent
variables (topics, other properties of the data you've added as part
of the model-building process) that best describe your data.

\subsection{Automatic Approaches}

However, automatic approaches do have several drawbacks.  They are
often restricted to specific platforms, are slower than inference ``by
hand'', and restrict the kinds of models you can explore.

Using a tool (no matter how wonderful) developed by someone else means
that you must accept their assumptions.  It may force you to use a
programming language that you're unfamiliar with, it may force you to
format your data in odd ways, or it may restrict you to operating
systems you don't often use (or can't afford).

% Church?

\paragraph{Stan}


\paragraph{Infer.Net}



\paragraph{Automatic Differentiation}



\subsection{Variational Inference}

While we focused on Gibbs sampling in Chapter~\ref{}, the other major
class of inference algorighms is variational inference.  Compared to
Gibbs sampling, variational inference is often considered to be
slightly more difficult to both derive and implement.

Variational inference is similar to expectation maximization
algorithms~\cite{}.  Expectation maximization algorithms find a
setting of latent variables $z$ and parameters $\theta$ that maximize
the data likelihood $p(x \g z, \theta)$.  First, start with some guess
of what the the latent variables might be $z_0$.  Then update the
parameters to be
\begin{equation}
  \arg \max_{\theta} p(x \g z, \theta),
\label{eq:em}
\end{equation}
the parameters that maximize the likelihood.  Then, given those
parameters, compute the expected value of the latent variables $z$ to
compute the next iteration of latent variables $z$.

Expectation maximization is a useful tool for inference when the model
distribution $p$ is simple enough to solve Equation~\ref{eq:em}
directly.  However, for many models (including topic models), this is
not feasible.  Variational inference solves this intractability by
adding its namesake variational distribution $q(z)$.

A variational distribution is over the model's latent variables.
Although it is a distribution over the same variables, it is typically
simpler.  For example, in common approaches for topic models~\citep{}, the
distribution is fully factorized.  For example, the true distribution
over latent variables is
\begin{equation}

\end{equation}
while the variation distribution is
\begin{equation}

\end{equation}

Instead of maximizing the data likelihood, variational inference
optimizes a different likelihood function: minimizing the Kulbeck-Leibler
\emph{divergence} between the distributions $p$ and $q$.  This is
equivalent to optimizing a lower bound of the data likelihood,
\begin{equation}
.
\end{equaiton}

\paragraph{Chosing a Variational Distribution}

On one hand, you want to choose a variational distribution that is
close to the true distribution over the latent variables.  In fact, if
you choose $q$ so that it is \emph{equal} to $p$, variational
inference reduces to expectation maximization.

However, this often comes at a cost: more complicated computations or
a more difficult (or even impossible) derivation.  You might not be
able to solve the equations to update individual variational
parameters or the calculations might be more difficult: the more
dependencies in the variational distribution, the more terms you must
consider.  When there are latent variables for every token in many
documents, this can quickly explode.

Often a fully-factored variational distribution is a good choice.
After implementing a fully-factorized variational distribution, a
reasearcher should carefully monitor the objective function (Equation~\ref{}).  Ideally
it will quickly increase and reach a stable (local) optimum
(Figure~\ref{}, left).  However,
if it does not (Figure~\ref{}, right), then it could be that there are
coupled variables that are not well served by the variational
distribution.

Coupled variables need to change together, and a fully-factorized
distribution forces them to change in sequence.

\paragraph{Deriving the Objective Function}

\subsection{Gibbs Sampling}

Deriving conditional distributions

\section{Debugging}

\subsection{Synthetic Data}

\subsection{Updates}

\section{Validating Whether it Worked}

\subsection{Baselines}

\subsection{Metrics}

\section{Publishing}

\subsection{Venues}

\subsection{What to Discuss}
