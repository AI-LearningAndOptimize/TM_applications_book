
\chapter{The What and Wherefore of Topic Models}
\label{ch:intro}

Imagine that you're an intrepid reporter with an amazing scoop: you have
twenty-four hours of exclusive access three decades of e-mails sent within a
corrupt corporation.  You know there's dirt and scandal there, but it's been
well-concealed by the corporation's political friends.  How are you going to
understand this haystack well enough to explain it to your devoted readers under
such a tight deadline?

\section{Tell me about your haystack}

Unlike the vignette above, interacting with large text data sets is often posed
as a needle in a haystack problem.  The user is confronted with a large,
undifferentiated collection of documents: the entire web, a hard drive, or all
communications within a company.  The poor user---faced with documents that
would take a decade to read---is looking for a single needle: a document (or at
most a handful of documents) that matches what the user is looking for.  This
could be a ``smoking gun'' e-mail, the document that best represents a
concept~\citep{Salton-68} or the answer to question~\citep{Hirschman-01}.

These questions are important.  The sub-discipline of information retrieval is
built upon systematizing, solving, and evaluating this problem.  Google's empire
is built on the premise of users typing a few keywords into a search engine box
and seeing quick, consistent search results.  However, this is not the only
problem that confronts those interacting with large text datasets.

A different, but related problem is \emph{understanding} large document
collections, common in science policy~\citep{talley-11}, journalism, and the
humanities~\citep{moretti-13}.  There isn't just one precious silver needle in
the haystack.  At the risk of abusing the metaphor, \emph{sometimes you care
  about the straw}.  Instead of looking for a smoking gun alerting to you some
crime an evil company committed, perhaps you're looking for a sin of
omission: did this company never talk about diversity in it's workforce?
Instead of a single answer to a question, perhaps you're looking for a diversity
of responses: what are the different ways that people account for rising income
inequality.  Instead of looking for one document, perhaps you want to answer
population level statistics: what proportion of Twitter users have ever talked
about gun violence?

At first, might seem that answering these questions require building an
extensive ontology or categorization scheme.  For every new corpus, define all
of the buckets that a document could fit into, have some librarians and
archivists put each document into the correct buckets, perhaps automate the
process with some supervised machine learning, and then collect summary
statistics when you're done.

Obviously, such laborious processes are possible---they've been done for X, Y,
and Z---and remain an important part of social science, information science,
library science, and machine learning.  But these processes aren't always
possible, fast, or even desireable.  First, they obviously require a significant
investment of time and resources.  Even creating the \emph{list} of categories
is a difficult task and requires careful delibreration and calibration.  It's
possible that a particular question does not warrant the time or effort: the
\oe{}vre of a minor author (only of interest to a few), or the tweets of a day
(not relevant tomorrow).

Topic models allow us to answer these big picture questions quickly and cheaply.
Topic models provide a framework for understanding these document collections
for both humans or computers.  This survey explores the ways that humans and
computers make sense of document collections through tools called topic models.
For readers already comfortable with topic models, feel free to skip this
chapter; we'll mostly cover the definitions and implementations of topic models.

\section{What is a Topic Model}

Returning to our motivating example, consider the e-mails from Enron, the prototypical
evil corporation of the turn of the century.

Topic models get their name from the topics they discover from text.
We focus on an example from a corpus of scientific literature.

Examples of topics.

Conventions of labeling topics.

Allocations of documents to the topics.

\section{Foundations}

Topic models began with a linear algebra
approach~\citep{deerwester-90}.  While these approaches have seen a
resurgence in recent years~\citep{anandkumar-12:lda,arora-13}, we focus
on probabilistic approaches, which are intuitive, work well, and allow
for easy modification (as we see later in \dots).

Probabilistic models begin with a generative story.

The prototypical topic model is latent Dirichlet
allocation~\citep{blei-03}.  Dirichlet distributions encode sparsity
and the intuitions about how documents are formed and how works are
used.

Topics are distributions over words.

Document allocations are distributions over topics.

The generative story is a fiction~\citep{box-87}; it may not be needed
to understand the organization of document collections.

\section{Inference}

Distinction between prior and posterior.  You can skip this section if
you want.

There are many flavors of algorithms for posterior inference: message
passing~\citep{zeng-13}, variational inference~\citep{blei-03}, gradient
descent~\citep{hoffman-10}, and Gibbs sampling~\citep{griffiths-04}.

We focus on Gibbs sampling, which is simple, intuitive, and
fast~\citep{yao-09}.

We present the results of Gibbs sampling without derivation, which are
well described elsewhere.

Gibbs sampling begins with random initialization.

You then change the topic assignments for each word. Example based on
our scientific dataset.

From these data, you can then get the topics and the document
allocations.

\section{Implementations}

Many implementations are available.

Mallet is fast and widely used~\citep{mallet}.  This is where you
should probably start, in our biased opinion.

Variational inference is the other major option, but requires more
preprocessing~\citep{blei-03,vw}.

Other options require clusters~\citep{Narayanamurthy-11,zhai-12}.

Other frameworks allow you to specify arbitrary generative
models~\citep{stan-software:2014}.

\section{Rest of this Survey}

Brief discussion of the rest of the survey.

The next chapter returns to the distinction between high level
overviews and finding a needle in a haystack.  We show how a high
level overview can help users and algorithms find documents of
interest.