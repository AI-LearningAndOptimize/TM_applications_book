
\chapter{The What and Wherefore of Topic Models}
\label{ch:intro}

Imagine that you're an intrepid reporter with an amazing scoop: you have
twenty-four hours of exclusive access three decades of e-mails sent within a
corrupt corporation.  You know there's dirt and scandal there, but it's been
well-concealed by the corporation's political friends.  How are you going to
understand this haystack well enough to explain it to your devoted readers under
such a tight deadline?

\section{Tell me about your haystack}

Unlike the vignette above, interacting with large text data sets is often posed
as a needle in a haystack problem.  The user is confronted with a large,
undifferentiated collection of documents: the entire web, a hard drive, or all
communications within a company.  The poor user---faced with documents that
would take a decade to read---is looking for a single needle: a document (or at
most a handful of documents) that matches what the user is looking for.  This
could be a ``smoking gun'' e-mail, the document that best represents a
concept~\citep{Salton-68} or the answer to question~\citep{Hirschman-01}.

These questions are important.  The sub-discipline of information retrieval is
built upon systematizing, solving, and evaluating this problem.  Google's empire
is built on the premise of users typing a few keywords into a search engine box
and seeing quick, consistent search results.  However, this is not the only
problem that confronts those interacting with large text datasets.

A different, but related problem is \emph{understanding} large document
collections, common in science policy~\citep{talley-11}, journalism, and the
humanities~\citep{moretti-13}.  There isn't just one precious silver needle in
the haystack.  At the risk of abusing the metaphor, \emph{sometimes you care
  about the straw}.  Instead of looking for a smoking gun alerting to you some
crime an evil company committed, perhaps you're looking for a sin of
omission: did this company never talk about diversity in it's workforce?
Instead of a single answer to a question, perhaps you're looking for a diversity
of responses: what are the different ways that people account for rising income
inequality.  Instead of looking for one document, perhaps you want to answer
population level statistics: what proportion of Twitter users have ever talked
about gun violence?

At first, might seem that answering these questions require building an
extensive ontology or categorization scheme.  For every new corpus, define all
of the buckets that a document could fit into, have some librarians and
archivists put each document into the correct buckets, perhaps automate the
process with some supervised machine learning, and then collect summary
statistics when you're done.

Obviously, such laborious processes are possible---they've been done for X, Y,
and Z---and remain an important part of social science, information science,
library science, and machine learning.  But these processes aren't always
possible, fast, or even desireable.  First, they obviously require a significant
investment of time and resources.  Even creating the \emph{list} of categories
is a difficult task and requires careful delibreration and calibration.  It's
possible that a particular question does not warrant the time or effort: the
\oe{}vre of a minor author (only of interest to a few), or the tweets of a day
(not relevant tomorrow).

Topic models allow us to answer these big picture questions quickly and cheaply.
Topic models provide a framework for understanding these document collections
for both humans or computers.  This survey explores the ways that humans and
computers make sense of document collections through tools called topic models.
For readers already comfortable with topic models, feel free to skip this
chapter; we'll mostly cover the definitions and implementations of topic models.

\section{What is a Topic Model}

Returning to our motivating example, consider the e-mails from Enron, the prototypical
evil corporation of the turn of the century.  Imagine that you're an
investigative reporter, the first to get your hands on Enron e-mails.  You know
that wrongdoing happened, but you don't know who did it or how it was planned
and carried out.  You have suspicions (e.g., around the California spot market),
but you are curious to know if there are other skeletons in the closet, and
you're highly motivated to find them.

So you run a topic model on the data.  True to its name, a topic model gives you
``topics'', collections of words that make sense together.  Looking at the Enron
e-mails, we can see topics about X, Y, and Z (Figure~\ref{fig:enron_topics}).

\begin{figure}
% TODO: Figure of Enron topics
  \caption{Topic model applied to Enron dataset}
  \label{fig:enron_topics}
\end{figure}


When we call a topic about X, that's a \textit{post hoc} label applied by humans
(more on this in Chapter~\ref{sec:display}).  A topic only gives you a jumbled
``bag of words''; it remains the responsibility of the human consumer of topic
models to go further and make sense of these piles of straw.

\begin{figure}
% TODO: Figure of Enron document and allocations to topics
  \caption{Example document from the Enron corpus and its association to
    topics.}
  \label{fig:enron_doc}
\end{figure}

Making sense of one of these piles requires actually reading the document, which
topic models also provide.  Each topic is associated with individual documents.
For example, the document in Figure~\ref{fig:enron_doc} is about X, which
combines Topic~Y and Topic~Z.  If we get a sense that Topic~Z is of interest, we
can explore deeper to find other documents
(Figure~\ref{fig:enron_topic_allocation}).

\begin{figure}
% TODO: Figure of Enron document and allocations to topics
  \caption{The documents associated with an Enron topic on X.}
  \label{fig:enron_topic_allocation}
\end{figure}

\section{Foundations}

\begin{figure}
% TODO: Figure of matrix story of topic modeling
  \caption{A matrix formulation of finding topics for a dataset.  While the
    original formulation of topic modeling approaches, we focus on probabilistic
    techniques in the rest of this survey.}
  \label{fig:matrix_topics}
\end{figure}

Topic models began with a linear algebra approach~\citep{deerwester-90} called
latent semantic analysis (\abr{lsa}): find the best low rank approximation of a
document-term matrix (Figure~\ref{fig:matrix_topics}).  While these approaches
have seen a resurgence in recent years~\citep{anandkumar-12:lda,arora-13}, we
focus on probabilistic approaches, which are intuitive, work well, and allow for
easy modification (as we see later in many of our later chapters).

\begin{figure}
% TODO: Figure of Dirichlet, multinomial, and discrete distributions: their
% parameters, their equations, what their draws look like
  \caption{}
  \label{fig:distribution_examples}
\end{figure}

\subsection{Probabilistic Building Blocks}

Probabilistic models begin with a generative story: a recipe listing a sequence of random events
that creates the dataset you're trying to explain.
Figure~\ref{fig:generative_distributions} lists some of the key players in these
stories, how they're parameterized and what their draws look like.  Let's
briefly discuss them, as we'll use them to build a wide variety of topic models
later.

\paragraph{Gaussian} If you know any probability distribution already, it's the
Gaussian.  It does not have a role in the most basic topic models that we'll
discuss here, but it will later (e.g., Chapter~\ref{sec:science_fields}).  We do
include it because it's a useful point of comparison against the other
distributions we're using (since it's perhaps the easiest to understand and best
known).

A Gaussian distribution is a distribution over all real numbers (e.g., $0.0, 0.5,
-4.2, \pi$, \dots).  You can ask it to spit out a number, and it will give you
some real number between negative infinity and positive infinity.  But not all
numbers have equal probability.  Gaussian distributions are parameterized by a
mean $\mu$ and variance $\sigma$.  Most samples from the distribution will be
near the mean $\mu$; how close is determined by the variance: higher variances
will cause the samples to be more spread out.

\paragraph{Discrete}

While Gaussian distributions are over a continuous space, documents are
combinations of discrete words.\footnote{An emerging trend in natural language
  processing research is to view words as embedded in a continuous space.  We
  discuss these ``representation learning'' approaches and their connection to
  topic modeling in Section~\ref{sec:future}.}   Thus, we need a distribution
over discrete sets.

A useful metaphor for thinking about discrete distributions is a weighted die.
The number of faces on the die is its dimension, and each face is a distinct
outcome.  Each outcome has its own probability of how likely that outcome is;
this is the parameter of a discrete distribution
(Figure~\ref{fig:distribution_examples}).

Topic models are described by discrete distributions (sometimes called
multinomial distributions) over words and topics.  The distribution over words
is called the topic distribution; each of the topics gives higher weights to
some words more than others (e.g., in topic X from the Enron corpus, ``foo'' has
higher probability).  Each document also has an ``allocation'' for each topic:
documents are about a small handful of topics, and most documents have very low
weights for most of the possible topics.

\paragraph{Dirichlet}

Although discrete distributions are the star players in topic models, they are
the end of the story.  The story actually begins with Dirichlet distributions.
Dirichlet distributions are distributions that produce discrete distributions.
Like the Gaussian distribution, they have parameters analagous to a mean and
variance.  The mean is called the ``base measure'' $\tau$ and is the expected
value of the Dirichlet: what you'd get if you averaged many draws from the
Dirichlet.  The concentration parameter $\alpha$ controls how far away draws are
from the base measure.

If $\alpha$ is very large, then the draws from a Dirichlet will be very close to
$\tau$ (Figure~\ref{fig:dirichlet_sparsity}, left).  If $\alpha$ is small, however,
something more interesting happens: the discrete distributions become sparse
(Figure~\ref{fig:dirichlet_sparsity}, right).  A sparse distribution is a
distribution where only a few values have high probability and most are small.

Because topic models are meant to reflect real documents, modeling sparsity
correctly is important.  When a person sits down to write a document, they only
write about a handful of topics.  They don't write about every possible topic
under the sun, and the sparsity of Dirichlet distributions is the probabilistic
tool that encodes this intuition.

\section{Latent Dirichlet Allocation}

We now have all the tools we need to tell the complete story of the prototypical
modern topic model: latent Dirichlet allocation~\citep{blei-03}.  Latent
Dirichlet allocation\footnote{The name \abr{lda} is a play on \abr{lsa}, its
  non-probabilistic forerunner (latent semantic analysis).  Latent because we
  use probabilistic inference to infer missing probabilistic pieces of the
  generative story.  Dirichlet because of the Dirichlet parameters encoding
  sparsity.  Allocation because the Dirichlet distribution encodes the prior for
  each document's allocation over topics.} posits a ``generative process'' about
how the data came to be.  We assemble the probabilistic pieces to tell
this story about generating topics and how those topics are used to create
diverse documents.

% TODO: add an example of how the Enron corpus gets built from the generative story

\paragraph{Generating Topics}

The first part of the story is to create the topics.  The user specifies that
there are $K$ distinct topics.  Each of the $K$ topics is drawn from a Dirichlet
distribution with a uniform base distribution and concentration parameter
$\lambda$: $\phi_k \sim \dir{\lambda {\bm u}}$.  The discrete distribution
$\phi_k$ has a weight for every word in the vocabulary.

\paragraph{Document Allocations}

Document allocations are distributions over topics for each document.  This
encodes what a document is about; the sparsity of the Dirichlet distribution's
concentration parameter $\alpha$ ensures that the document will only be about a
few topics.  Each document has a discrete distribution over topic: $\theta_d \sim
\dir{\alpha {\bm u}}$.

\paragraph{Words in Context}

Now that we know what each document is about, we need to actually create the
words that appear in the document.  We assume\footnote{It's possible to model
  this in the generative story as well, e.g., with a Poisson distribution.
  However, we often do not care about document \emph{lengths}---only what the
  document is about---so we can usually ignore this part of the story.} that
there are $N_d$ words in document $d$.  For each word $n$ in the
document $d$, we first choose a {\bf topic assignment} $z_{d,n} \sim
\disc{\theta_d}$.  This is one of the $K$ topics that tells us which topic the
word token is from, but not what the word is.

To select which word we'll see in the document, we draw from the a discrete
distribution again.  Given a words topic assignment $z_{d,n}$, we draw from that
topic to select the word: $w_{d,n} \sim \phi_{z_{d,n}}$.  The topic assignment
tells you what the word is about, and then this selects which distribution over
words we'll use to generate the word (Figure~\ref{fig:generative-ex}).

It goes without saying that the generative story is a fiction~\citep{box-87}.
Nobody is sitting down with dice to decide what to type in on their keyboard.
We use this story because it is \emph{useful}.  This fanciful story about randomly
choosing a topic for each word can help us because if we assume this generative
process, we can work backwards to find the topics that explain how a document
collection was created: every word, every document, gets associated with these
underlying topics.

This fantasy helps us order our document collection: by assuming this story, we
can discover \emph{topics} (which certainly don't exist) so we can understand
the common themes that people use to write documents.  As we'll see in later
chapters, slight tweaks of this generative story allow us to tell uncover more
complicated structures: how authors prefer specific topics, how topics change
over time, or how topics can be used across languages.

\section{Inference}

Given a generative model and some data, the process of uncovering the hidden
probabilistic pieces of the generative story is called \emph{inference}.  More
concretely, it is a recipie for generating algorithms to go from data to
\emph{topics that explain a dataset}.

There are many flavors of algorithms for posterior inference: message
passing~\citep{zeng-13}, variational inference~\citep{blei-03}, gradient
descent~\citep{hoffman-10}, and Gibbs sampling~\citep{griffiths-04}.  All of
these algorithms have their advocates and reasons you should use them.  However,
that discussion is better left for other venues.  In this survey, we focus
on Gibbs sampling, which is simple, intuitive, and fast~\citep{yao-09}.

We present the results of Gibbs sampling without derivation, which---along with
the history of its origin in statistical physics---are well described
elsewhere.\footnote{TODO: mention gsftu, griffiths, resources for the
  derivation} We use a variety of Gibbs sampling called \emph{collapsed} Gibbs
sampling, which ignores some of the pieces of the generative story: it ignores
the topics and the document allocations to only focus on the topic assignments.
Some relatively straightforward math allows you to recreate the topics and
document allocations if you only know the topic assignments.

\subsection{Random Variables}

\paragraph{Topic Assignments}

Recall that every individual token gets a topic assignment.  For example,
an instance of the word ``compilation'' might be in a business topic in one
document and in an arts topic in another document.  It's even possible that the
same word might be assigned to different topics in the same document, as each
instance of the word has its own topic assignment.  Because topic models care
about \emph{global} properties, we'll use aggregate statistics derived from
these topic assignments.

\paragraph{Document Allocation} The document allocation is a distribution over
the topics for each document; in other words, it says how popular each topic is
in a document.  If we count up how often a document uses a topic, this gives us
an idea of the popularity.  Let's define $\dc{d}{i}$ as the number of times
document~$d$ uses topic~$i$.  Clearly, this is larger for more popular topics;
however, it's not a probability because it is larger than one.  We can make it a
probability by dividing by the number of words in a document
\begin{equation}
\frac{\dc{d}{i}}{\sum_k \dc{d}{k}},
\label{eq:theta_ml}
\end{equation}
but this is problematic because it can sometimes give us zero and ignores the
influence of the Dirichlet distribution; a better estimate is\footnote{To be
  technical, Equation~\ref{eq:theta_ml} is a maximum likelihood estimate and
  Equation~\ref{eq:theta_map} is the maximum \textit{a posteriori}, which
  incorporates the influence of both the prior and the data.}
\begin{equation}
\theta_{d,i} \approx \frac{\dc{d}{i} + \alpha_i}{\sum_k \dc{d}{k} + \alpha_k}.
\label{eq:theta_map}
\end{equation}
It's important that this is never zero because we don't want it to be impossible
for a topic to get used in a particular document.  This helps the sampler
explore more of the possible combinations.

\paragraph{Topics}

Each topic is a distribution over words.  To understand what a topic is about,
we look at the profile of all of the tokens that have been assigned to that
topic.  We estimate the probability of a word in a topic as
\begin{equation}
\phi_{i,v} \approx \frac{\tc{i}{v} + \beta_v}{\sum_w \tc{i}{w} + \beta_w},
\label{eq:phi_map}
\end{equation}
where $\beta$ is the Dirichlet parameter for the topic distribution.


\subsection{Algorithm}

The algorithm for learning a topic model is only based on the topic assignments,
but we'll use our estimates for the topics $\phi_k$ and the documents $\theta_d$
discussed above.  We begin by setting these topic assignments randomly: if we
have $K$ topics, each word has equal chance to be associated with any of the
topics.

You then change the topic assignments for each word. Example based on
our Enron dataset.

The equation for the probability of assigning a word to a particular topic
combines these two factors\footnote{To be theoretically correct, it is important
not to include the count associated with the token you are currently sampling in
these counts, which becomes more clear if the probability is written as
$p(z_{d,n}=j\,|\,z_{d,1}\dots z_{d,n-1},z_{d,n+1}\dots z_{d,N_d}, w_{d,n})$ to
show the dependence on the topic assignments of \emph{all other} token but not
this token.}
\begin{align}
p(z_{d,n}=j\,|) = \theta_d
\phi_j = \left(\frac{\dc{d}{i} + \alpha_i}{\sum_k \dc{d}{k} + \alpha_k} \right) \left( \frac{\tc{i}{w_{d,n}} + \beta_v}{\sum_w \tc{i}{w} +
    \beta_w} \right).
\end{align}


From these data, you can then get the topics and the document
allocations.

\subsection{Implementations}

Many implementations are available.

Mallet is fast and widely used~\citep{mallet}.  This is where you
should probably start, in our biased opinion.

Variational inference is the other major option, but requires more
preprocessing~\citep{blei-03,vw}.

Other options require clusters~\citep{Narayanamurthy-11,zhai-12}.

Other frameworks allow you to specify arbitrary generative
models~\citep{stan-software:2014}.

\section{Rest of this Survey}

Brief discussion of the rest of the survey.

The next chapter returns to the distinction between high level
overviews and finding a needle in a haystack.  We show how a high
level overview can help users and algorithms find documents of
interest.