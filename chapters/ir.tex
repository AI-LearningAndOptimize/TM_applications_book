\chapter{Information Retrieval}
\label{ch:ir}

Information Retrieval (IR) systems aim to retrieve relevant documents by comparing query and document texts. Users start with their information need in the form of queries. Early IR systems treat both the query and documents as ``bag of words'', retrieve and rank the documents by measuring the word overlap between queries and documents.

\jbgcomment{It would be good to contrast a little bit with the role that topic models serve in exploring document collections (discussed in the intro); i.e., you don't know what you're looking for.  This chapter focuses instead on when you do know what you're looking for (in the form of a query).  We may even want to call this ``traditional information retrieval'' or ``query-based IR''.}

However, the ability of this direct and simple matching is always limited. Words with similar meaning or in different forms should also be considered as matched instead of being ignored. \textbf{Language modeling} has been one of the most popular frameworks to capture such semantic relationships. Also, humans would also use background knowledge to interpret and understand the queries and ``add'' missing words~\citep{wei-07}, which provides another way often referred as \textbf{query expansion} to improve the retrieval and ranking results.

Both directions can be pursued by learning and discovering the semantic relations between words and further the semantic relations between queries and documents. Topic models, which describe each topic using weighted words and model each document as a distribution over all topics, is an effective way \jbgcomment{how?} to capture such semantic relations~\citep{deerwester-90,hofmann-99a}.

In this chapter, we briefly introduce the information retrieval framework and two major directions of applying topic models to improve the information retrieval results: document language modeling \citep{Lu-2011,wei-06} and query expansion~\citep{Park-2009,Andrzejewski-2011}.

\section{Document Language Modeling in IR}

The language modeling approach~\citep{croft-03,PonteCroft,song-99} is one of the main frameworks for using topic models in IR systems, since it has been shown to be effective probabilistic framework for studying information retrieval problem~\citep{PonteCroft,berger-99}.

A statistical language model is to estimate the probability of word sequences, denoted as $p(w_1,w_2,\cdots,w_n)$. In practice, the statistical language model is often approximated by N-gram models. A unigram model assumes each word in the sequence is independent, and is denoted as,
\begin{align}
p(w_1,w_2,\cdots,w_n) = p(w_1)p(w_2) \cdots p(w_n)
\end{align}
A trigram model assumes the probability of the current word only depends on the previous two words, and it is represented as,
\begin{align}
p(w_1,w_2,\cdots,w_n)=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)\cdots p(w_n|w_{n-2},w_{n-1})
\end{align}

\jbgcomment{would be good to add backpointers to previous chapter}

In the application of information retrieval, the queries are generated by a probabilistic language model based on a document~\citep{zhai-01}. More specifically, each document is viewed as a language sample, and a language model for each document is estimated based on document terms. Then the probability of generating the query is estimated by each document language model. The probability of a query is computed by multiplying the probabilities of generating each query term using different document language model, and the documents are ranked based on the probability.

Given a document sample $d$, a straightforward way to estimate the probability of generating a word $w$ is to use the maximum likelihood estimation as,
\begin{align}
p_{\texttt{ml}}(w|d) = \frac{n_{d,w}}{n_{d,\cdot}}
\label{eq:ir_mse}
\end{align}
where $n_{d,w}$ is the term frequency of word $w$ in document $d$, and $n_{d,\cdot}$ is the total number of tokens in document $d$. Then the probability of the given query $q$ can be computed by,
\begin{align}
p(q|d) = \prod_{w \in q} p(w|d) = \prod_{w \in q} \frac{n_{d,w}}{n_{d,\cdot}}
\end{align}

Then the documents are ranked based on this probability. Higher probability implies the corresponding document is more relevant to the given query~\citep{song-99}. However, a document is often too small to cover all the terms in the query. The probability of a missing term is zero, which means the probability of the whole query is zero and causes problems for ranking documents. This data sparsity problem can be fixed by smoothing, which will be discussed next.

\subsection{Smoothing Document Language Models}

\jbgcomment{Is this important to talk about here?  I worry that it might be a distraction.  will this connect directly to topic models?}

This data sparsity problem can be fixed by smoothing, which allocates some non-zero probability to the missing terms. There are two major directions for smoothing: \textbf{interpolation}~\citep{Jelinek-1980,mackay95dirichlet,Ney-1994,PonteCroft,zhai-01} and \textbf{backoff}~\citep{katz-87,song-99}. The interpolation-based method discount the counts of the seen words and distribute the extra counts to both seen words and unseen words. Three popular and simple smoothing methods are summarized by \cite{zhai-01}.

\paragraph{Jlinek-Mercer method}

The Jlinek-Mercer method~\cite{Jelinek-1980} is a linear interpolation of the maximum likelihood model in a document with the model based on the whole corpus, and a coefficient $\lambda$ is used to combine the two parts.
\begin{align}
p(w|d) = (1 - \lambda) p_{\texttt{ml}(w|d)} + \lambda p(w|\mathcal{C})
\end{align}
where $\mathcal{C}$ denotes the whole corpus. This simple mixture solves the data sparsity problem. For terms that occur in the document $d$, the maximum likelihood estimator (Equation~\ref{eq:ir_mse}) is not accurate given the limited size of a document, thus it is smoothed with the more reliable corpus level probability. For the missing terms in the document $d$, the probability is not zero any more, but fall back to the corpus level probability. This smoothing method has been explored by \cite{PonteCroft} and \cite{song-99} in information retrieval task, except \cite{PonteCroft} explored a weighted product version instead of a linear interpolation.
\begin{align}
p(w|d) = p_{\texttt{ml}}(w|d)^{(1 - \lambda) } \times p(w|\mathcal{C})^{\lambda}
\label{eq:lm-jr}
\end{align}
But in general, the linear interpolation is more popular since the resulting probability is still normalized~\citep{song-99}.

\paragraph{Bayesian Smoothing using Dirichlet Priors}

A language model can be viewed as a multinomial distribution, thus it can be smoothed by applying the Dirichlet distribution as the conjugate prior~\citep{mackay95dirichlet}. The smoothing model is,
\begin{align}
p(w|d) = \frac{n_{d,w} + \mu p(t|\mathcal{C})}{\sum_{v \in V} n_{d,v} + \mu}
\end{align}
where the Dirichlet prior is decided by parameter $\mu$ and the corpus-level probabilities $p(v|\mathcal{C})$ as follows,
\begin{align}
(\mu p(v_1 | \mathcal{C}), \mu p(v_2 | \mathcal{C}), \cdots, \mu p(v_n | \mathcal{C}))
\label{eq:lm-bs}
\end{align}

\paragraph{Absolute Discounting}

Absolute discounting is to allocate some probability mass from the seen words to the unseen words~\cite{Ney-1994}. Normally, a constant is subtracted from their actual counts for the seen words. This model is given by,
\begin{align}
p(w|d) = \frac{\max(n_{d,w} - \delta, 0)}{\sum_{v \in V} n_{d,v}} + \sigma p(w|\mathcal{C})
\end{align}
where $\delta \in [0,1]$ is the discount constant and $\sigma = \delta n'_{d,\cdot} / n_{d, \cdot}$, $n_{d, \cdot}$ is the total number of tokens in document $d$, and $n'_{d,\cdot}$ is the number of unique terms in document $d$.

However, the problem with interpolation smoothing is that it may distribute too much extra count to the word with high count. An alternative smoothing strategy is based on backoff techniques. The main idea is to trust the maximum likelihood estimation for high count words, discount and  redistribute mass only for the less common words~\cite{zhai-01}. The Katz smoothing~\citep{katz-87} is a well known back-off smoothing method in speech recognition tasks.

If we represent the interpolation-based smoothing method as $p_s(w) = p_{\texttt{dml}}(w|d) + \alpha p(w|\mathcal{C})$ for seen words and $p_u(w) = \alpha p(w|\mathcal{C})$ for unseen words, the backoff version is then given by $p_s(w) = p_{\texttt{dml}}(w|d)$ and $p_u(t) = \frac{\alpha p(w|\mathcal{C})}{1-\sum_{w' \in V: n_{w'}>0} \alpha p(w'|\mathcal{C})}$. Follow this rule, the above three interpolation methods can also have their back-off versions.

%\cite{song-99} has applied the Good-Turing estimate to smooth each document language model in information retrieval task.

\subsection{Applying Topic Models to Document Language Models}

Topic models, which model each document as a mixture of topics and each topic as a mixture of words, offer an interesting framework to model documents in information retrieval. \cite{wei-06} introduce Latent Dirichlet Allocation (LDA) to learn the relationship between query words and documents. While topic models learn the document-topic distribution and the topic-word distributions, the conditional probability of a query word $w$ given a document $d$ is computed as marginalizing all topics:
\begin{align}
p_{\texttt{lda}}(w|d) = \sum_{z=1}^K p(w|z, \hat{\phi}) p(z | \hat{\theta}, d)
\end{align}
where $\hat{\theta}$ is the posterior estimate for the document-topic distribution $\theta$; $\hat{\phi}$ is the posterior estimate for the topic-word distribution $\phi$; $K$ is the total number of topics.

While topic models introduce new topic perspective information to information retrieval, it is a little too sparse to be used alone~\citep{wei-06}. Thus \cite{wei-06} further propose to combine the LDA-based document model with the original smoothed document model~(Equation~\ref{eq:lm-jr}) through a linear model,
\begin{align}
p(w|d) = \omega ((1 - \lambda) p_{\texttt{ml}(w|d)} + \lambda p(w|\mathcal{C})) + (1 - \omega) p_{\texttt{lda}}(w|d)
\end{align}
where $\omega$ is the coefficient which combines the LDA-based document model with the general smoothed language model.

Following \cite{wei-06}, \cite{Lu-2011} further evaluate the performance of applying topic models into the language model framework. Instead of combining with the language model with Jlinek-Mercer smoothing~(Equation~\ref{eq:lm-jr}), \cite{Lu-2011} smooth the language model with the Bayesian smoothing~(Equation~\ref{eq:lm-bs}), and the final linear combination with topic models is as,
\begin{align}
p(w|d) = \omega \frac{n_{d,w} + \mu p(t|\mathcal{C})}{\sum_{v \in V} n_{d,v} + \mu}  + (1 - \omega) p_{\texttt{lda}}(w|d)
\end{align}

\jbgcomment{I don't know this work as well, but I haven't seen enough here to convince me that this is a good idea.  How does it help performance, why is it a good idea?  Can you give an example where it works well?}

While using different smoothing strategies, both approaches~(\citep{wei-06} and \citep{Lu-2011}) apply topic models to connect the query words with documents through hidden topics. As a result, better semantic relationships between query words and documents can be learned and the document language models are improved.

\section{Query Expansion in IR}

The document language models in Information Retrieval~\citep{PonteCroft} have been attempting to model the query generation process based on the document models. However, a big problem is that these models abandon modeling the query-document relevance explicitly~\citep{Lavrenko-2001}, which is very important in traditional Information Retrieval task.

In fact, queries, which are normally brief and using informal languages from users, diverse significantly from the language in documents~\citep{Muller-2009}. This semantic gap or lexical gap can result in poor query-document relevance, even though the document is quite relevant from the users' view point. This is due to users, who add ``missing'' or potentially related words implicitly in their minds and then consider the query-document relevance.

Query expansion simulates the similar process. Query expansion normally analyzes the relationships between the query words and other words, and tries to find potential related words so that the original query is better represented and better query-document relevance can be obtained. This section reviews the classic query expansion frameworks in information retrieval and then introduce the related works about using topic models for query expansion.

\subsection{Learning Query-Word Relationships for Query Expansion}

There are two main steps for query expansion. The first step is to find the relationships between queries and words and select the top related words to expand the query. The second step is to apply the expanded queries for ranking and compute the final ranking relevance scores. We start with the first step. Two major directions have been explored: query language model~\citep{zhai-01b} and relevance model~\citep{Lavrenko-2001}.

\paragraph{Query Language Model}

To learn the query-word relationship, \cite{zhai-01b} build up a query language model to estimate the proabbility $p(w|q)$ of a word $w$ given a query $q$. However, it is not easy to learn a good query language model since the query content is too limited.

\cite{zhai-01b} further propose to use both the query content and the relevant documents (sometimes referred as feedback documents or clicked documents) to estimate the query language model. Let $\hat{\theta}_{\mathcal{F}}$ be the estimated query language model based on the relevant documents, the combined query model $\hat{\theta}_{Q'}$ can be computed by,
\begin{align}
\hat{\theta}_{Q'} = (1 - \mu)\hat{\theta}_{Q} + \mu \hat{\theta}_{\mathcal{F}}
\end{align}

A simple way to estimate the feedback query model $\hat{\theta}_{\mathcal{F}}$ is a unigram language model $\theta$ which generates each word in $\mathcal{F}$ independently. However, most documents contain not only the query relevant information, but also the background information. As a result, \cite{zhai-01b} propose to generate a relevant document by a mixture model, which combines a query model $p(w|\theta_Q)$ with a collection language model $p(w|\mathcal{C})$. Thus the log-likelihood of the relevant document is,
\begin{align}
\log p(\mathcal{F}|\theta) = \sum_i \sum_w c(w, d_i) \log((1-\lambda)p(w|\theta_Q) + \lambda p(w|\mathcal{C}))
\end{align}
where $c(w, d_i)$ is count of word $w$ in document $d$. The related parameters $\theta$ are estimated by the EM algorithm (more details can be found in \cite{zhai-01b}). By combing with the information from the relevant documents, the query language model is more reobust, thus query-word relationships are better represented.

\paragraph{Relevance Model}

Different from the query language model approach~\citep{zhai-01b}, \cite{Lavrenko-2001} assume both the query and the relevant documents are random samples from an unknown relevance model $R$. Given the query $q = q_1 \cdots q_n$, they approximate the probability $p(w|R)$ based on the observed query words sequence $q_1 \cdots q_n$ as the following,
\begin{align}
p(w|R) \approx p(w|q_1 \cdots q_n) = \frac{p(w,q_1 \cdots q_n)}{p(q_1 \cdots q_n)}
\end{align}

To estimate the joint probability $p(w,q_1 \cdots q_n)$, \cite{Lavrenko-2001} assumes the word $w$ and the query $q$ are sampled independently from the same distribution, e.g., from a unigram distribution, then the joint probability can be computed as,
\begin{align}
p(w, q) = p(w,q_1 \cdots q_n) &= \sum_{D \in \mathcal{C}} p(D) p(w,q_1 \cdots q_n | D) \\
&= \sum_{D \in \mathcal{C}} p(D) p(w|D) p(q | D)
\end{align}

Then the multinomial distribution $p(w|q)$ for a given query $q$ can be computed as follows,
\begin{align}
\label{eq:rm_qe}
p(w|q) = \frac{p(w, q)}{p(q)} = \sum_{D \in \mathcal{C}} p(w|D) p(D|q)
\end{align}

Both the query language model and the relevance model capture the relationship between the query and other words, based on which the top related words can be selected for query expansion.

\subsection{Ranking Relevance with Query Expansion}

Given the related words for query expansion, the next question is how to apply the expanded queries for computing the final ranking relevance score. The combination can happen either before or after the relevance score is computed.

\cite{zhai-01b} combines the expanded model $\hat{\theta}_{\mathcal{F}}$ with the original query model $\hat{\theta}_{Q}$ as one query language model $\hat{\theta}_{Q'} $ as the following equation and then compute the relevance scores.
\begin{align}
\hat{\theta}_{Q'} = (1 - \mu)\hat{\theta}_{Q} + \mu \hat{\theta}_{\mathcal{F}}
\end{align}

There are multiple ways to compute the final relevance score. Given the document language model and the updated query language model,  \cite{Lafferty-2001} and \cite{zhai-01b} introduce the KL-divergence retrieval model to measure the distance between the query model and document model. More formally, given a query $q$ generated fro a query model $p(q|\theta_Q)$, and a document generated from a document model $p(d|\theta_D)$, and $\theta_Q$ and $\theta_D$ are estimated by the query language model and document language model as $\hat{\theta}_Q$ and $\hat{\theta}_D$ respectively, the KL-divergence of document $d$ with respect to $q$ is calculated as:
\begin{align}
D(\hat{\theta}_Q || \hat{\theta}_D) = -\sum_w p(w|\hat{\theta}_Q) \log p(w | \hat{\theta}_D) + \texttt{cons}(q)
\end{align}

Different from \cite{zhai-01b}, \cite{Lavrenko-2001} compute the relevance score using the original query and the expanded query respectively, and then linearly combine the two scores. Thus the final query-document relevance $\hat{s}_d(q)$ is computed as,
\begin{align}
\label{eq:rm_qe_comb}
\hat{s}_d(q) = \mu s_d(e) + (1-\mu)s_d(q)
\end{align}
where $s_d(q)$ is the relevance between the original query $q$ and documents $d$, and $s_d(e)$ is relevance between the expanded query terms $e$ and document $d$.

\subsection{Applying Topic Models For Query Expansion}

\cite{Yi-2009} follow the relevance model approach and introduce the topics to query expansion. The intuition is to learn the topics with high probabilities in a query and then expand the query with the terms with high weights in these topics. In order to do so, they first learn a topic model for queries, from which the probability $p_{\texttt{TM}}(k|q)$ of a topic $k$ in a query $q$ is learned. Then the query-word relevance $p(w|q)$ is computed based on topics as follows,
\begin{align}
\label{eq:query_word_prob}
p_{\texttt{TM}}(w|q) = \sum_k p_{\texttt{TM}}(w|k) p_{\texttt{TM}}(k|q)
\end{align}

This approach extracts the query-word relevance from topic models directly. The high relevant words for the given query $q$ can be selected and used for query expansion to improve the ranking results as Equation~\ref{eq:rm_qe_comb}.

In addition to applying topic models on queries, \cite{Yi-2009} also apply the topics extracted from the relevant documents for query expansion. They train topic models with the top-k documents retrieved by a query, and extract the query-word relationships based on the Equation~\ref{eq:query_word_prob}.

\cite{Yi-2009} also apply topic models to improve the relevance model in Equation~\ref{eq:rm_qe}, which is further used for query expansion.
\begin{align}
p(w|q) = \sum_{D \in \mathcal{C}} (\omega p(w|D) + (1 - \omega)p_{\texttt{TM}}(w|D,q))p(D|q)
\end{align}
where $\omega$ is a constant weight to combine the original relevance model and the topic-based relevance model.
\begin{align}
p_{\texttt{TM}}(w|D,q) = \sum_k p(w|k) p(k|D,q)
\end{align}
where,
\begin{align}
p(k|D,q) &= \frac{p(k,D,q)}{p(D,q)}  = \frac{p(k)p(D|k)p(q|k)}{p(D,q)} \\
&= \frac{p(k|D)p(q|k)}{p(q|D)} \approx p(k|D)p(q|k)
\end{align}
where $p(k|D)$ is the topic probability in document $D$, and $p(q|k)$ is the probability of generating a query $q$ given the topic $k$.

\cite{Park-2009} also apply topic models for query expansion. Based on the topics extracted from topic models, they compute the probabilistic relationships of each term pair $(t_x, t_y)$,
\begin{align}
p(t_x|t_y, \alpha) &= \sum_i p(t_x, z_i | t_y, \alpha) \\
&= \sum_i p(t_x | z_i, t_y, \alpha) p(z_i | t_y, \alpha) \\
&= \sum_i p(t_x|z_i, \alpha) p(z_i|t_y, \alpha)
\end{align}
where $p(t_x|z_i, \alpha)$ is the topic-word distribution which can be learned from topic models, and $p(z_i|t_y, \alpha)$ can be computed as,
\begin{align}
p(z_i|t_y, \alpha) &= \frac{p(t_y|z_i,\alpha)p(z_i|\alpha)}{p(t_y|\alpha)} \\
&= \frac{p(t_y|z_i,\alpha)p(z_i|\alpha)}{\sum_j p(t_y, z_j|\alpha)} \\
&= \frac{p(t_y|z_i,\alpha)p(z_i|\alpha)}{\sum_j p(t_y|z_i,\alpha)p(z_i|\alpha)}
\end{align}
where $p(t_y|z_i,\alpha)$ is the topic-word distributions, and \cite{Park-2009} also show that $p(z_i|\alpha) = \frac{\alpha_i}{\sum_k \alpha_k}$.
%\begin{align}
%p(z_i|\alpha) &= \int p(z_i, \theta | \alpha) d\theta \\
%&= \int p(z_i | \theta, \alpha) p(\theta | \alpha) d\theta \\
%&= \int p(z_i | \theta) p (\theta | \alpha) d\theta \\
%& = \mathcal{E}_{p(\theta | \alpha)} [\theta_i]\\
%&= \frac{\alpha_i}{\sum_k \alpha_k}
%\end{align}
As a result, we have,
\begin{align}
p(z_i|t_y, \alpha) &= \frac{p(t_y|z_i,\alpha) \frac{\alpha_i}{\sum_k \alpha_k} }{\sum_j p(t_y|z_i,\alpha) \frac{\alpha_j}{\sum_k \alpha_k}} \\
&= \frac{p(t_y|z_i,\alpha) \alpha_i}{\sum_j p(t_y|z_i,\alpha) \alpha_j}
\end{align}

The final probabilistic relationships of each term pair  can be represented as,
\begin{align}
p(t_x|t_y, \alpha) = \frac{\sum_i p(t_x|z_i, \alpha) p(t_y|z_i,\alpha) \alpha_i }{\sum_j p(t_y|z_j,\alpha) \alpha_j}
\end{align}

\jbgcomment{Again, would be good to have a clear example here}

Once they obtain this term relationships, they choose the top related terms as the expanded terms $e$ for the given query $q$, and the final document ranking score is computed as Equation~\ref{eq:rm_qe_comb}.

\subsection{Bilingual Topic Models for Query Expansion}

\cite{Gao-2012} further introduce the bilingual topic models~\citep{Gao-2011} for query expansion. They assume the search query and its relevant Web documents share a common distribution of topics, but use different vocabularies to express these topics. Thus in their models, queries and documents have different topic-word distributions $\phi_z^Q$ and $\phi_z^D$ respectively. To generate a query $q$, a document-topic distribution $\theta^Q$ is drawn from a Dirichlet prior, and a topic $z$ is sampled from $\theta^Q$, then a query term $q_i$ is sampled from $\phi_z^Q$. To generate a document term, a topic $z$ is firstly sampled from $\theta^Q$, the same document-topic distribution as the query, and then a document term $d_i$ is sampled from document topic-word distribution $\phi_z^D$. In this way, documents and queries are conntected through the hidden topics, even though their vocabularies (topic-word distributions) are different. By summing over all possible topics, the relationship between document term $e$ and query $q$ can be computed as,
\begin{align}
p(e|q) = \sum_z p(e|\phi_z^D) p(z | \theta_q)
\end{align}
The paramters can be learned by standard EM algorithm. The top related terms in the relevant documents can be used for query expansion.

\jbgcomment{Should connect to your multilngual chapter}

\subsection{Getting Interactive Feedback using Topic Models}

Relevance feedback involves the users in the retrieval process to improve the ranking result set~\citep{Rocchio-1971}. The basic idea is to ask users to give feedback on the relevance of documents in an initial set of retrieval results, and users' feedback on relevance is further used to improve the ranking results. This process can go through one or more iterations.

Different from the other approaches, \cite{Andrzejewski-2011} present a new framework for obtaining and exploiting user feedback at the latent topic level. They learn the latent topics from the whole corpus and construct meaningful topic representations. At query time, they decide which latent topics are potentially relevant and present the topic representations along keyword search results. When a user select a latent topic, the original query is expaned with the top words in this selected topic, and the search results are refined.

In order to construct more meaningful topic representations, \cite{Andrzejewski-2011} extract multiple features, such as word probability, topic posterior, PMI, etc., and then select a single best topic word for each topic as the topic label. They also identify the statistical significant bigrams and trigrams to further better represent the topics. In addition, they restore the capitalization for the n-grams to ease users' interpretation of topics.

\jbgcomment{Some of these ideas are discussed in other chapters (interactivity, labeling), would be good to add links}

At run time, given a query and its top ranked documents, \cite{Andrzejewski-2011} select the top two topics from the each of the top two ranked documents as the enriched topic set. They further study the topic relations and identify the top two topics with the highest covariance with each enriched topics, which is defined as the related topics. They also filter out the in-coherent topics by using PMI scores. For the remaining coherent topics, \cite{Andrzejewski-2011} display their unigram, bigram and trigram representations to users along the search results of the original query. Once a user select a latent topic, the search results are updated by expanding the original query with the top words in this select topic.

%\section{Summary}
% Perhaps it would be nice to mention e-discovery?
%Transition to next chapter: what if you care about recall and understanding
\jbgcomment{needs conclusion and text leading into next chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{comment}
\cite{PonteCroft} first propose the language model in information retrieval. They treat the generation of queries as a random process and use the probabilities from documents to model the query generations. However, they think the maximum likelihood estimator (Equation~\ref{eq:ir_mse}) is not accurate, given the limited size of a document. So they propose to use the averaged probability across the corpus to smooth the maximum likelihood estimator. The averaged probability $p_{\texttt{avg}}(t)$ is computed as,
\begin{align}
p_{\texttt{avg}}(t) = \frac{\sum_{d_{t \in d}} p_{\texttt{ml}}(t|d)}{\texttt{df}_t}
\end{align}
where $\texttt{df}_t$ is the document frequency of term $t$. They then combine the maximum likelihood estimator with this averaged probability across the corpus,
\begin{align}
p(t|d) &= p_{\texttt{ml}}(t|d)^{(1-R_{t,d})} \times p_{\texttt{avg}}(t)^{R_{t,d}}
\end{align}
where $R_{t,d}$ is the risk for a term $t$ in document $d$ using the geometric distribution, computed as,
\begin{align}
R_{t,d} &= (\frac{1}{1+\bar{f_t}}) \times (\frac{\bar{f}_t}{1+\bar{f}_t})^{\texttt{tf}_{t,d}} \\
\bar{f}_t &= \frac{\sum_{d_{t \in d}} \texttt{tf}_{t,d}}{\texttt{df}_t}
\end{align}
where $\texttt{tf}_{t,d}$ is the term frequency of term $t$ in document $d$; $\bar{f}_t$ is the mean term frequency of term $t$ in all documents where term $t$ occurs.

For the missing terms in the current document, \cite{PonteCroft} estimate their probabilities based on the ratio of the term frequency in all documents and the total number of tokens, normally referred as background probability. The final probability is estimated by,
\begin{align}
p(t|d) =
\begin{cases}
p_{\texttt{ml}}(t|d)^{(1-R_{t,d})} \times p_{\texttt{avg}}(t)^{R_{t,d}},~~~~~~~~&\text{if $tf_{t,d}$ > 0}\\
\frac{\sum_d \texttt{tf}_{t,d}}{\sum_{t} \sum_d \texttt{tf}_{t,d}}. &\text{otherwise}
\end{cases}
\end{align}

Following \cite{PonteCroft}, \cite{song-99} further explore the language models for information retrieval based on a range of data smoothing techniques. They first apply the Good-Turing estimate to smooth each document language model, which allocates some probability to the missing terms as,
\begin{align}
p_{\texttt{GT}}(t|d) = \frac{(tf+1)S(N_{tf+1})}{N_d S(N_{tf})}
\end{align}
where $tf$ is the term frequency of term $t$; $N_{tf}$ is the number of terms with frequency $tf$ in a document; $S(N_{tf})$ is a smoothed function.

The second approach that \cite{song-99} proposed is to expand a document model with the corpus model, which is very similar to the idea in \cite{PonteCroft}, except that they are using a weighted sum and a weight $w$ can be learned,
\begin{align}
p(t|d) = w \times p_{document}(t|d) + (1-w) \times p_{corpus}(t)
\end{align}

\cite{song-99} further propose to combine unigrams and bigrams to further smooth the probability,
\begin{align}
p(t_{i-1},t_i|d) = w \times p_1(t_i|d) + (1-w) \times p_2(t_{i-1},t_i | d)
\end{align}
which can be further extended to include trigrams as well.
\end{comment}



\begin{comment}

\chapter{Information Retrieval Structure}
\label{ch:ir_structure}

Topic models, such as Latent Semantic Analysis (LSA) by \citep{deerwester-90} and probabilistic Latent Semantic Indexing~\cite{hofmann-99a}, can capture the semantic relations between documents and queries~\citep{wei-07}.

Topic models have been applied into the information retrieval framework to improve ranking results. Probabilistic language modeling~\citep{croft-03} is a common formalism that incorporates information from topic models.

In this chapter, we briefly introduce the information retrieval framework and the semantic relations between queries and documents. We also present the latent semantic analysis and probabilistic latent semantic indexing models, and how they are combined into information retrieval framework through language modeling.

\section{Semantic Relations in Information Retrieval}

Information Retrieval (IR) systems aim to retrieve relevant documents by comparing query and document texts.

From computer's view, Documents are usually treated as ``bags of words'', and documents are retrieved and ranked by measuring the word overlap. This is consistent with topic models.

Topic models use those bags of words to build abstractions (topics), however.

However, humans would use background knowledge to interpret and understand the queries and ``add'' missing words~\citep{wei-07}, akin to query expansion.

\jbgcomment{You never define what's a semantic relation here}

To more accurately retrieve related documents, the semantic relations between queries and documents are needed to improve the ranking results. Topic models, which describe each topic using weighted words and model each document as a distribution over all topics, is an effective way to capture such semantic relations~\citep{deerwester-90,hofmann-99a}.

\section{Topic Models in IR}

Latent Semantic Analysis (LSA) proposed by \citet{deerwester-90} makes use of dimensionality reduction to capture the semantic relations among words and documents.  (Briefly review, note distinction with probabilistic approaches.)

It has been successfully applied into the information retrieval systems through automatic indexing with LSA (Latent Semantic Indexing, LSI) \citep{deerwester-90,dumais-95}.

The probabilistic Latent Semantic Indexing (pLSI), introduced by \citet{hofmann-99a}, is an aspect model which associates an unobserved class (topic) variable with each observed word occurrence. pLSI provides a better fit to text data than LSI, thus quickly gained acceptance in IR systems.

\section{Applying Topic Models into IR}

The language modeling approach~\citep{croft-03,PonteCroft,song-99} is the main framework for using topic models in IR systems, since it has been shown to be effective probabilistic framework for studying information retrieval problem~\citep{PonteCroft,berger-99}.

The basic language modeling framework is to compute the model likelihood of documents for generating the queries. Topic models, which represent documents with topics, offer a new and interesting means to model documents~\citep{wei-07}. A probability mixture model and a term model with back-off smoothing are presented to integrate topic models in this section.

A probability mixture model combines multiple different probability distributions to integrate different factors in IR for query representation or document representation~\citep{miller-99,zhai-01,liu-04}.

A term model with back-off smoothing~\citep{katz-87} is another popular framework to use topic models in IR systems, where a term model is learned for each term in a document and the back-off smoothing~\citep{katz-87} is applied.

% Perhaps it would be nice to mention e-discovery?

Transition to next chapter: what if you care about recall and understanding

\end{comment}