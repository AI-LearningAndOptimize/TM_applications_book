\chapter{Information Retrieval}
\label{ch:ir}

Information Retrieval (IR) systems aim to retrieve relevant documents by comparing query and document texts. Users start with their information need in the form of queries. Early IR systems treat both the query and documents as ``bag of words'', retrieve and rank the documents by measuring the word overlap between queries and documents. 

However, the ability of this direct and simple matching is always limited. Words with similar meaning or in different forms should also be considered as matched instead of being ignored. Also, humans would also use background knowledge to interpret and understand the queries and ``add'' missing words~\citep{wei-07}, which provides another way often referred as query expansion to improve the retrieval and ranking results.

Both directions can be pursued by learning and discovering the semantic relations between words and further the semantic relations between queries and documents. Topic models, which describe each topic using weighted words and model each document as a distribution over all topics, is an effective way to capture such semantic relations~\citep{deerwester-90,hofmann-99a}.

In this chapter, we briefly introduce the information retrieval framework and two major directions of applying topic models to improve the information retrieval results: document language modeling \citep{Lu-2011,wei-06} and query expansion~\citep{Park-2009,Andrzejewski-2011}.

%\section{Probabilistic Information Retrieval}

%Latent Semantic Analysis (LSA) proposed by \citet{deerwester-90} makes use of dimensionality reduction to capture the semantic relations among words and documents.  (Briefly review, note distinction with probabilistic approaches.)

%It has been successfully applied into the information retrieval systems through automatic indexing with LSA (Latent Semantic Indexing, LSI) \citep{deerwester-90,dumais-95}.

%The probabilistic Latent Semantic Indexing (pLSI), introduced by \citet{hofmann-99a}, is an aspect model which associates an unobserved class (topic) variable with each observed word occurrence. pLSI provides a better fit to text data than LSI, thus quickly gained acceptance in IR systems.

\section{Document Language Modeling in IR}

The language modeling approach~\citep{croft-03,PonteCroft,song-99} is one of the main frameworks for using topic models in IR systems, since it has been shown to be effective probabilistic framework for studying information retrieval problem~\citep{PonteCroft,berger-99}.

A statistical language model is to estimate the probability of word sequences, denoted as $p(w_1,w_2,\cdots,w_n)$. In practice, the statistical language model is often approximated by N-gram models. A unigram model assumes each word in the sequence is independent, and is denoted as,
\begin{align}
p(w_1,w_2,\cdots,w_n) = p(w_1)p(w_2) \cdots p(w_n)
\end{align}
A trigram model assumes the probability of the current word only depends on the previous two words, and it is represented as,
\begin{align}
p(w_1,w_2,\cdots,w_n)=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)\cdots p(w_n|w_{n-2},w_{n-1})
\end{align}

In the application of information retrieval, the queries are generated by a probabilistic language model based on a document~\citep{zhai-01}. More specifically, each document is viewed as a language sample, and a language model for each document is estimated based on document terms. Then the probability of generating the query is estimated by each document language model. The probability of a query is computed by multiplying the probabilities of generating each query term using different document language model, and the documents are ranked based on the probability. 

Given a document sample $d$, a straightforward way to estimate the probability of generating a word $w$ is to use the maximum likelihood estimation as,
\begin{align}
p_{\texttt{ml}}(w|d) = \frac{n_{d,w}}{n_{d,\cdot}}
\label{eq:ir_mse}
\end{align}
where $n_{d,w}$ is the term frequency of word $w$ in document $d$, and $n_{d,\cdot}$ is the total number of tokens in document $d$. Then the probability of the given query $q$ can be computed by,
\begin{align}
p(q|d) = \prod_{w \in q} p(w|d) = \prod_{w \in q} \frac{n_{d,w}}{n_{d,\cdot}}
\end{align}

Then the documents are ranked based on this probability. Higher probability implies the corresponding document is more relevant to the given query~\citep{song-99}. However, a document is often too small to cover all the terms in the query. The probability of a missing term is zero, which means the probability of the whole query is zero and causes problems for ranking documents. This data sparsity problem can be fixed by smoothing, which will be discussed next.

\subsection{Smoothing Document Language Models}

This data sparsity problem can be fixed by smoothing, which allocates some non-zero probability to the missing terms. There are two major directions for smoothing: \textbf{interpolation}~\citep{Jelinek-1980,mackay95dirichlet,Ney-1994,PonteCroft,zhai-01} and \textbf{backoff}~\citep{katz-87,song-99}. The interpolation-based method discount the counts of the seen words and distribute the extra counts to both seen words and unseen words. Three popular and simple smoothing methods are summarized by \cite{zhai-01}. 

\paragraph{Jlinek-Mercer method}

The Jlinek-Mercer method~\cite{Jelinek-1980} is a linear interpolation of the maximum likelihood model in a document with the model based on the whole corpus, and a coefficient $\lambda$ is used to combine the two parts. 
\begin{align}
p(w|d) = (1 - \lambda) p_{\texttt{ml}(w|d)} + \lambda p(w|\mathcal{C})
\end{align}
where $\mathcal{C}$ denotes the whole corpus. This simple mixture solves the data sparsity problem. For terms that occur in the document $d$, the maximum likelihood estimator (Equation~\ref{eq:ir_mse}) is not accurate given the limited size of a document, thus it is smoothed with the more reliable corpus level probability. For the missing terms in the document $d$, the probability is not zero any more, but fall back to the corpus level probability. This smoothing method has been explored by \cite{PonteCroft} and \cite{song-99} in information retrieval task, except \cite{PonteCroft} explored a weighted product version instead of a linear interpolation. 
\begin{align}
p(w|d) = p_{\texttt{ml}}(w|d)^{(1 - \lambda) } \times p(w|\mathcal{C})^{\lambda}
\label{eq:lm-jr}
\end{align}
But in general, the linear interpolation is more popular since the resulting probability is still normalized~\citep{song-99}.

\paragraph{Bayesian Smoothing using Dirichlet Priors}

A language model can be viewed as a multinomial distribution, thus it can be smoothed by applying the Dirichlet distribution as the conjugate prior~\citep{mackay95dirichlet}. The smoothing model is,
\begin{align}
p(w|d) = \frac{n_{d,w} + \mu p(t|\mathcal{C})}{\sum_{v \in V} n_{d,v} + \mu}
\end{align}
where the Dirichlet prior is decided by parameter $\mu$ and the corpus-level probabilities $p(v|\mathcal{C})$ as follows,
\begin{align}
(\mu p(v_1 | \mathcal{C}), \mu p(v_2 | \mathcal{C}), \cdots, \mu p(v_n | \mathcal{C}))
\label{eq:lm-bs}
\end{align}

\paragraph{Absolute Discounting}

Absolute discounting is to allocate some probability mass from the seen words to the unseen words~\cite{Ney-1994}. Normally, a constant is subtracted from their actual counts for the seen words. This model is given by,
\begin{align}
p(w|d) = \frac{\max(n_{d,w} - \delta, 0)}{\sum_{v \in V} n_{d,v}} + \sigma p(w|\mathcal{C})
\end{align}
where $\delta \in [0,1]$ is the discount constant and $\sigma = \delta n'_{d,\cdot} / n_{d, \cdot}$, $n_{d, \cdot}$ is the total number of tokens in document $d$, and $n'_{d,\cdot}$ is the number of unique terms in document $d$. 

However, the problem with interpolation smoothing is that it may distribute too much extra count to the word with high count. An alternative smoothing strategy is based on backoff techniques. The main idea is to trust the maximum likelihood estimation for high count words, discount and  redistribute mass only for the less common words~\cite{zhai-01}. The Katz smoothing~\citep{katz-87} is a well known back-off smoothing method in speech recognition tasks. 

If we represent the interpolation-based smoothing method as $p_s(w) = p_{\texttt{dml}}(w|d) + \alpha p(w|\mathcal{C})$ for seen words and $p_u(w) = \alpha p(w|\mathcal{C})$ for unseen words, the backoff version is then given by $p_s(w) = p_{\texttt{dml}}(w|d)$ and $p_u(t) = \frac{\alpha p(w|\mathcal{C})}{1-\sum_{w' \in V: n_{w'}>0} \alpha p(w'|\mathcal{C})}$. Follow this rule, the above three interpolation methods can also have their back-off versions.

%\cite{song-99} has applied the Good-Turing estimate to smooth each document language model in information retrieval task.

\subsection{Applying Topic Models to Document Language Models}

Topic models, which model each document as a mixture of topics and each topic as a mixture of words, offer an interesting framework to model documents in information retrieval. \cite{wei-06} introduce Latent Dirichlet Allocation (LDA) to improve the language model as follows,
\begin{align}
p_{\texttt{lda}}(w|d) = \sum_{z=1}^K p(w|z, \hat{\phi}) p(z | \hat{\theta}, d)
\end{align}
where $\hat{\theta}$ is the posterior estimate for the document-topic distribution $\theta$; $\hat{\phi}$ is the posterior estimate for the topic-word distribution $\phi$; $K$ is the total number of topics.

While topic models introduce new topic perspective information to information retrieval, it is a little too sparse to be used alone~\citep{wei-06}. Thus \cite{wei-06} further propose to combine the LDA-based document model with the original smoothed document model~(Equation~\ref{eq:lm-jr}) through a linear model,
\begin{align}
p(w|d) = \omega ((1 - \lambda) p_{\texttt{ml}(w|d)} + \lambda p(w|\mathcal{C})) + (1 - \omega) p_{\texttt{lda}}(w|d)
\end{align}
where $\omega$ is the coefficient which combines the LDA-based document model with the general smoothed language model.

Following \cite{wei-06}, \cite{Lu-2011} further evaluate the performance of applying topic models into the language model framework. Instead of combining with the language model with Jlinek-Mercer smoothing~(Equation~\ref{eq:lm-jr}), \cite{Lu-2011} smooth the language model with the Bayesian smoothing~\ref{eq:lm-bs}, and the final linear combination with topic models is as,
\begin{align}
p(w|d) = \omega \frac{n_{d,w} + \mu p(t|\mathcal{C})}{\sum_{v \in V} n_{d,v} + \mu}  + (1 - \omega) p_{\texttt{lda}}(w|d)
\end{align}




%The basic language modeling framework is to compute the model likelihood of documents for generating the queries. Topic models, which represent documents with topics, offer a new and interesting means to model documents~\citep{wei-07}. A probability mixture model and a term model with back-off smoothing are presented to integrate topic models in this section.

%A term model with back-off smoothing~\citep{katz-87} is another popular framework to use topic models in IR systems, where a term model is learned for each term in a document and the back-off smoothing~\citep{katz-87} is applied.

\section{Relevance Models for Query Expansion in IR}

The document language models in Information Retrieval~\citep{PonteCroft} have been attempting to model the query generation process based on the document models. However, a big problem is that these models abandon modeling the query-document relevance explicitly~\citep{Lavrenko-2001}, which is very important in traditional Information Retrieval task.

\subsection{Classic Relevance Models}

The classic probabilistic models of information retrieval, for example, probability ranking principle~\citep{Robertson-1997}, argue that the optimal ranking performance is achieved if the documents are ranked by the odds of their being observed in the relevant class $p(D|R) / p(D/N)$, where $D$ is the document and $N$ is a set of documents non-relevant to current document $D$. If we assume the words in the documents are independent, we can rank the documents by,
\begin{align}
\frac{p(D|R)}{p(D|N)} \sim \prod_{w \in D} \frac{p(w|R)}{p(w|N)}
\end{align}
where $p(w|R)$ and $p(w|N)$ define the probabilities of observing a word $w$ in relevant and non-relevant document sets respectively. 

However, given a query and a large collection of documents, without the relevance labels, it is hard to estimate $p(w|R)$ since we have no training data. \cite{Lavrenko-2001} assume both the query and the relevant documents are random samples from an unknown relevance model $R$. Given the query $q = q_1 \cdots q_n$, they approximate the probability $p(w|R)$ based on the observed query words sequence $q_1 \cdots q_n$ as the following,
\begin{align}
p(w|R) \approx p(w|q_1 \cdots q_n) = \frac{p(w,q_1 \cdots q_n)}{p(q_1 \cdots q_n)}
\end{align}

To estimate the joint probability $p(w,q_1 \cdots q_n)$, \cite{Lavrenko-2001} explore two methods, where the first one assumes the word $w$ and the query $q$ are sampled from the same distribution and the second one assumes they are sampled using different mechanisms. Here we only introduce the first simpler method to illustrate. Assuming $w$, query $q$ and the documents are all sampled independently from a unigram distribution document mode, the joint probability can be computed as,
\begin{align}
p(w,q_1 \cdots q_n) &= \sum_{D \in \mathcal{C}} p(D) p(w,q_1 \cdots q_n | D) \\
&= \sum_{D \in \mathcal{C}} p(D) p(w|D) p(q | D)
\end{align}

Once we get this joint probability, $p(w|R)$ can be easily computed, and the documents can be ranked based on $p(D|R) / p(D/N)$~\citep{Robertson-1997}. 

\subsection{Relevance Models for Query Expansion}

In addition to this ranking framework~\citep{Robertson-1997}, this relevance model~\citep{Lavrenko-2001} has also been used for query expansion, which is to calculate a multinomial distribution $p(w|q)$ for a given query $q$ as follows,
\begin{align}
p(w|q) = \frac{p(w, q)}{p(q)} = \sum_{D \in \mathcal{C}} p(w|D) p(D|q)
\end{align}

In order to avoid bias, the final query-document relevance $\hat{s}_d(q)$ is normally a linear combination of the relevance $s_d(q)$ between the original query $q$ and documents $d$, and the relevance between the expanded query terms $e$ and documents $d$ as follows,
\begin{align}
\label{eq:rm_qe}
\hat{s}_d(q) = \mu s_d(e) + (1-\mu)s_d(q) 
\end{align}

\subsection{Applying Topic Models to Improve Relevance Models}

\cite{Yi-2009} follow this relevance model approach and introduce the topics to query expansion. The intuition is to learn the topics with high probabilities in a query and then expand the query with the terms with high weights in these topics. In order to do so, they first learn a topic model for queries, from which the probability $p_{\texttt{TM}}(k|q)$ of a topic $k$ in a query $q$ is learned. Then the query-word relevance $p(w|q)$ is computed based on topics as follows,
\begin{align}
\label{eq:query_word_prob}
p_{\texttt{TM}}(w|q) = \sum_k p_{\texttt{TM}}(w|k) p_{\texttt{TM}}(k|q)
\end{align}

This approach extracts the query-word relevance from topic models directly. The high relevant words for the given query $q$ can be selected and used for query expansion to improve the ranking results as Equation~\ref{eq:rm_qe}.

Besides, \cite{Yi-2009} also apply topic models to improve the relevance model, which is further used for query expansion. 
\begin{align}
p(w|q) = \sum_{D \in \mathcal{C}} (\omega p(w|D) + (1 - \omega)p_{\texttt{TM}}(w|D,q))p(D|q)
\end{align}
where $\omega$ is a constant weight to combine the original relevance model and the topic-based relevance model. 
\begin{align}
p_{\texttt{TM}}(w|D,q) = \sum_k p(w|k) p(k|D,q)
\end{align}
where,
\begin{align}
p(k|D,q) &= \frac{p(k,D,q)}{p(D,q)}  = \frac{p(k)p(D|k)p(q|k)}{p(D,q)} \\
&= \frac{p(k|D)p(q|k)}{p(q|D)} \approx p(k|D)p(q|k)
\end{align}
where $p(k|D)$ is the topic probability in document $D$, and $p(q|k)$ is the probability of generating a query $q$ given the topic $k$. 

\cite{Park-2009} also apply topic models for query expansion. Based on the topics extracted from topic models, they compute the probabilistic relationships of each term pair $(t_x, t_y)$, 
\begin{align}
p(t_x|t_y, \alpha) &= \sum_i p(t_x, z_i | t_y, \alpha) \\
&= \sum_i p(t_x | z_i, t_y, \alpha) p(z_i | t_y, \alpha) \\
&= \sum_i p(t_x|z_i, \alpha) p(z_i|t_y, \alpha)
\end{align}
where $p(t_x|z_i, \alpha)$ is the topic-word distribution which can be learned from topic models, and $p(z_i|t_y, \alpha)$ can be computed as,
\begin{align}
p(z_i|t_y, \alpha) &= \frac{p(t_y|z_i,\alpha)p(z_i|\alpha)}{p(t_y|\alpha)} \\
&= \frac{p(t_y|z_i,\alpha)p(z_i|\alpha)}{\sum_j p(t_y, z_j|\alpha)} \\
&= \frac{p(t_y|z_i,\alpha)p(z_i|\alpha)}{\sum_j p(t_y|z_i,\alpha)p(z_i|\alpha)}
\end{align}
where $p(t_y|z_i,\alpha)$ is the topic-word distributions, and \cite{Park-2009} also show that $p(z_i|\alpha) = \frac{\alpha_i}{\sum_k \alpha_k}$. 
%\begin{align}
%p(z_i|\alpha) &= \int p(z_i, \theta | \alpha) d\theta \\
%&= \int p(z_i | \theta, \alpha) p(\theta | \alpha) d\theta \\
%&= \int p(z_i | \theta) p (\theta | \alpha) d\theta \\
%& = \mathcal{E}_{p(\theta | \alpha)} [\theta_i]\\
%&= \frac{\alpha_i}{\sum_k \alpha_k}
%\end{align}
As a result, we have,
\begin{align}
p(z_i|t_y, \alpha) &= \frac{p(t_y|z_i,\alpha) \frac{\alpha_i}{\sum_k \alpha_k} }{\sum_j p(t_y|z_i,\alpha) \frac{\alpha_j}{\sum_k \alpha_k}} \\
&= \frac{p(t_y|z_i,\alpha) \alpha_i}{\sum_j p(t_y|z_i,\alpha) \alpha_j}
\end{align}

The final probabilistic relationships of each term pair  can be represented as,
\begin{align}
p(t_x|t_y, \alpha) = \frac{\sum_i p(t_x|z_i, \alpha) p(t_y|z_i,\alpha) \alpha_i }{\sum_j p(t_y|z_j,\alpha) \alpha_j}
\end{align}

Once they obtain this term relationships, they choose the top related terms as the expanded terms $e$ for the given query $q$, and the final document ranking score is computed as Equation~\ref{eq:rm_qe}.

\section{Relevance Feedback for Query Expansion}

Relevance feedback involves the users in the retrieval process to improve the ranking result set~\citep{Rocchio-1971}. The basic idea is to ask users to give feedback on the relevance of documents in an initial set of retrieval results, and users' feedback on relevance is further used to improve the ranking results. This process can go through one or more iterations. 

\subsection{Ranking Framework for Feedback Models}

Generally, the retrieval problem is to learn the query and document representation respectively, and then match the query and documents by measuring the distance between the query representation and document representation. \cite{Lafferty-2001} introduce the KL-divergence retrieval model to measure the distance between the query model and document model.

More formally, given a query $q$ generated fro a query model $p(q|\theta_Q)$, and a document generated from a document model $p(d|\theta_D)$, and $\theta_Q$ and $\theta_D$ are estimated by the query language model and document language model as $\hat{\theta}_Q$ and $\hat{\theta}_D$ respectively, the KL-divergence of document $d$ with respect to $q$ is calculated as:
\begin{align}
D(\hat{\theta}_Q || \hat{\theta}_D) = -\sum_w p(w|\hat{\theta}_Q) \log p(w | \hat{\theta}_D) + \texttt{cons}(q)
\end{align}

The entropy of the query model $\texttt{cons}(q)$ can be dropped since it is only related with the query, and the minimum value is achieved when $\hat{\theta}_Q$ is identical to $\hat{\theta}_D$. Within this KL-divergence retrieval framework, the retrieval problem is to estimate the query language model $\hat{\theta}_Q$ and the document language model $\hat{\theta}_D$. While any language model can be used for queries and documents in principle, it is not easy to learn a good query language model since the query content is too limited. 

\cite{zhai-01b} propose to use both the query content and the relevant documents (feedback documents) to estimate the query language model. Let $\hat{\theta}_{\mathcal{F}}$ be the estimated query model based on the feedback documents, the combined query model $\hat{\theta}_{Q'}$ can be computed by,
\begin{align}
\hat{\theta}_{Q'} = (1 - \mu)\hat{\theta}_{Q} + \mu \hat{\theta}_{\mathcal{F}}
\end{align}

A simple way to estimate the feedback query model $\hat{\theta}_{\mathcal{F}}$ is a unigram language model $\theta$ which generates each word in $\mathcal{F}$ independently. However, most documents contain not only the query relevant information, but also the background information. As a result, \cite{zhai-01b} propose to generate a feedback document by a mixture model, which combines a query model $p(w|\theta_Q)$ with a collection language model $p(w|\mathcal{C})$. Thus the log-likelihood of the feedback document is,
\begin{align}
\log p(\mathcal{F}|\theta) = \sum_i \sum_w c(w, d_i) \log((1-\lambda)p(w|\theta_Q) + \lambda p(w|\mathcal{C}))
\end{align}
where $c(w, d_i)$ is count of word $w$ in document $d$. The related parameters $\theta$ are estimated by the EM algorithm (more details can be found in \cite{zhai-01b}). In this framework, a document $d$ is ranked based on the score of combining the query model $\hat{\theta}_Q$ with the feedback query model $\hat{\theta}_\mathcal{F}$, and then compute the KL-divergence of document $d$ with respect to the query $q$. 

\subsection{Topic Models for Feedback Models}

\cite{Yi-2009} apply the topics extracted from the feedback documents for query expansion. They train topic models with the top-k documents retrieved by a query, and extract the query-word relationships based on the Equation~\ref{eq:query_word_prob}.




\section{Summary}
% Perhaps it would be nice to mention e-discovery?

Transition to next chapter: what if you care about recall and understanding


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{comment}
\cite{PonteCroft} first propose the language model in information retrieval. They treat the generation of queries as a random process and use the probabilities from documents to model the query generations. However, they think the maximum likelihood estimator (Equation~\ref{eq:ir_mse}) is not accurate, given the limited size of a document. So they propose to use the averaged probability across the corpus to smooth the maximum likelihood estimator. The averaged probability $p_{\texttt{avg}}(t)$ is computed as,
\begin{align}
p_{\texttt{avg}}(t) = \frac{\sum_{d_{t \in d}} p_{\texttt{ml}}(t|d)}{\texttt{df}_t}
\end{align}
where $\texttt{df}_t$ is the document frequency of term $t$. They then combine the maximum likelihood estimator with this averaged probability across the corpus,
\begin{align}
p(t|d) &= p_{\texttt{ml}}(t|d)^{(1-R_{t,d})} \times p_{\texttt{avg}}(t)^{R_{t,d}}
\end{align}
where $R_{t,d}$ is the risk for a term $t$ in document $d$ using the geometric distribution, computed as,
\begin{align}
R_{t,d} &= (\frac{1}{1+\bar{f_t}}) \times (\frac{\bar{f}_t}{1+\bar{f}_t})^{\texttt{tf}_{t,d}} \\
\bar{f}_t &= \frac{\sum_{d_{t \in d}} \texttt{tf}_{t,d}}{\texttt{df}_t}
\end{align}
where $\texttt{tf}_{t,d}$ is the term frequency of term $t$ in document $d$; $\bar{f}_t$ is the mean term frequency of term $t$ in all documents where term $t$ occurs. 

For the missing terms in the current document, \cite{PonteCroft} estimate their probabilities based on the ratio of the term frequency in all documents and the total number of tokens, normally referred as background probability. The final probability is estimated by,
\begin{align}
p(t|d) = 
\begin{cases}
p_{\texttt{ml}}(t|d)^{(1-R_{t,d})} \times p_{\texttt{avg}}(t)^{R_{t,d}},~~~~~~~~&\text{if $tf_{t,d}$ > 0}\\
\frac{\sum_d \texttt{tf}_{t,d}}{\sum_{t} \sum_d \texttt{tf}_{t,d}}. &\text{otherwise}
\end{cases}
\end{align}

Following \cite{PonteCroft}, \cite{song-99} further explore the language models for information retrieval based on a range of data smoothing techniques. They first apply the Good-Turing estimate to smooth each document language model, which allocates some probability to the missing terms as,
\begin{align}
p_{\texttt{GT}}(t|d) = \frac{(tf+1)S(N_{tf+1})}{N_d S(N_{tf})}
\end{align}
where $tf$ is the term frequency of term $t$; $N_{tf}$ is the number of terms with frequency $tf$ in a document; $S(N_{tf})$ is a smoothed function. 

The second approach that \cite{song-99} proposed is to expand a document model with the corpus model, which is very similar to the idea in \cite{PonteCroft}, except that they are using a weighted sum and a weight $w$ can be learned,
\begin{align}
p(t|d) = w \times p_{document}(t|d) + (1-w) \times p_{corpus}(t)
\end{align}

\cite{song-99} further propose to combine unigrams and bigrams to further smooth the probability,
\begin{align}
p(t_{i-1},t_i|d) = w \times p_1(t_i|d) + (1-w) \times p_2(t_{i-1},t_i | d)
\end{align}
which can be further extended to include trigrams as well.
\end{comment}



\begin{comment}

\chapter{Information Retrieval Structure}
\label{ch:ir_structure}

Topic models, such as Latent Semantic Analysis (LSA) by \citep{deerwester-90} and probabilistic Latent Semantic Indexing~\cite{hofmann-99a}, can capture the semantic relations between documents and queries~\citep{wei-07}.

Topic models have been applied into the information retrieval framework to improve ranking results. Probabilistic language modeling~\citep{croft-03} is a common formalism that incorporates information from topic models.

In this chapter, we briefly introduce the information retrieval framework and the semantic relations between queries and documents. We also present the latent semantic analysis and probabilistic latent semantic indexing models, and how they are combined into information retrieval framework through language modeling.

\section{Semantic Relations in Information Retrieval}

Information Retrieval (IR) systems aim to retrieve relevant documents by comparing query and document texts.

From computer's view, Documents are usually treated as ``bags of words'', and documents are retrieved and ranked by measuring the word overlap. This is consistent with topic models.

Topic models use those bags of words to build abstractions (topics), however.

However, humans would use background knowledge to interpret and understand the queries and ``add'' missing words~\citep{wei-07}, akin to query expansion.

To more accurately retrieve related documents, the semantic relations between queries and documents are needed to improve the ranking results. Topic models, which describe each topic using weighted words and model each document as a distribution over all topics, is an effective way to capture such semantic relations~\citep{deerwester-90,hofmann-99a}.

\section{Topic Models in IR}

Latent Semantic Analysis (LSA) proposed by \citet{deerwester-90} makes use of dimensionality reduction to capture the semantic relations among words and documents.  (Briefly review, note distinction with probabilistic approaches.)

It has been successfully applied into the information retrieval systems through automatic indexing with LSA (Latent Semantic Indexing, LSI) \citep{deerwester-90,dumais-95}.

The probabilistic Latent Semantic Indexing (pLSI), introduced by \citet{hofmann-99a}, is an aspect model which associates an unobserved class (topic) variable with each observed word occurrence. pLSI provides a better fit to text data than LSI, thus quickly gained acceptance in IR systems.

\section{Applying Topic Models into IR}

The language modeling approach~\citep{croft-03,PonteCroft,song-99} is the main framework for using topic models in IR systems, since it has been shown to be effective probabilistic framework for studying information retrieval problem~\citep{PonteCroft,berger-99}.

The basic language modeling framework is to compute the model likelihood of documents for generating the queries. Topic models, which represent documents with topics, offer a new and interesting means to model documents~\citep{wei-07}. A probability mixture model and a term model with back-off smoothing are presented to integrate topic models in this section.

A probability mixture model combines multiple different probability distributions to integrate different factors in IR for query representation or document representation~\citep{miller-99,zhai-01,liu-04}.

A term model with back-off smoothing~\citep{katz-87} is another popular framework to use topic models in IR systems, where a term model is learned for each term in a document and the back-off smoothing~\citep{katz-87} is applied.

% Perhaps it would be nice to mention e-discovery?

Transition to next chapter: what if you care about recall and understanding

\end{comment}