\chapter{Information Retrieval}
\label{ch:ir}

Information Retrieval (IR) systems aim to retrieve relevant documents by comparing query and document texts. Users start with their information need in the form of queries. Early IR systems treat both the query and documents as ``bag of words'', retrieve and rank the documents by measuring the word overlap between queries and documents. 

However, the ability of this direct and simple matching is always limited. Words with similar meaning or in different forms should also be considered as matched instead of being ignored. Also, humans would also use background knowledge to interpret and understand the queries and ``add'' missing words~\citep{wei-07}, which provides another way often referred as \textbf{query expansion} to improve the retrieval and ranking results. 

Both directions can be pursued by learning and discovering the semantic relations between words and further the semantic relations between queries and documents. Topic models, which describe each topic using weighted words and model each document as a distribution over all topics, is an effective way to capture such semantic relations~\citep{deerwester-90,hofmann-99a}.

In this chapter, we briefly introduce the information retrieval framework and two major directions of applying topic models to improve the information retrieval results: query expansion~\citep{Park-2009,Andrzejewski-2011} and probabilistic language modeling \citep{Lu-2011,wei-06}.

%\section{Probabilistic Information Retrieval}

%Latent Semantic Analysis (LSA) proposed by \citet{deerwester-90} makes use of dimensionality reduction to capture the semantic relations among words and documents.  (Briefly review, note distinction with probabilistic approaches.)

%It has been successfully applied into the information retrieval systems through automatic indexing with LSA (Latent Semantic Indexing, LSI) \citep{deerwester-90,dumais-95}.

%The probabilistic Latent Semantic Indexing (pLSI), introduced by \citet{hofmann-99a}, is an aspect model which associates an unobserved class (topic) variable with each observed word occurrence. pLSI provides a better fit to text data than LSI, thus quickly gained acceptance in IR systems.

\section{Language Modeling in IR}

The language modeling approach~\citep{croft-03,PonteCroft,song-99} is one of the main frameworks for using topic models in IR systems, since it has been shown to be effective probabilistic framework for studying information retrieval problem~\citep{PonteCroft,berger-99}.

A statistical language model is to estimate the probability of word sequences, denoted as $p(w_1,w_2,\cdots,w_n)$. In practice, the statistical language model is often approximated by N-gram models. A unigram model assumes each word in the sequence is independent, and is denoted as,
\begin{align}
p(w_1,w_2,\cdots,w_n) = p(w_1)p(w_2) \cdots p(w_n)
\end{align}
A trigram model assumes the probability of the current word only depends on the previous two words, and it is represented as,
\begin{align}
p(w_1,w_2,\cdots,w_n)=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)\cdots p(w_n|w_{n-2},w_{n-1})
\end{align}

In the application of information retrieval, the queries are generated by a probabilistic language model based on a document~\cite{zhai-01}. More specifically, each document is viewed as a language sample, and a language model for each document is estimated based on document terms. Then the query is generated by each document language model. The probability of a query is computed by multiplying the probabilities of generating each query term using different document language model, and the documents are ranked based on the probability. Higher probability implies the corresponding document is more relevant to the given query~\cite{song-99}.

Given a document sample $d$, a straightforward way to estimate the probability of a term $t$ is to use the maximum likelihood estimation as below,
\begin{align}
p(t|d) = \frac{n_{d,t}}{n_{d,\cdot}}
\end{align}
where $n_{d,t}$ is the term frequency of term $t$ in document $d$, and $n_{d,\cdot}$ is the total number of tokens in document $d$. Then the probability of the given query can be computed. However, a document is often too small to cover all the terms in the query, and the probability of a missing term is zero, which means the probability of the whole query is zero and causes problems for ranking documents. 




The basic language modeling framework is to compute the model likelihood of documents for generating the queries. Topic models, which represent documents with topics, offer a new and interesting means to model documents~\citep{wei-07}. A probability mixture model and a term model with back-off smoothing are presented to integrate topic models in this section.

A term model with back-off smoothing~\citep{katz-87} is another popular framework to use topic models in IR systems, where a term model is learned for each term in a document and the back-off smoothing~\citep{katz-87} is applied.

\section{Query Expansion in IR}

A probability mixture model combines multiple different probability distributions to integrate different factors in IR for query representation or document representation~\citep{miller-99,zhai-01,liu-04}.


\section{Summary}
% Perhaps it would be nice to mention e-discovery?

Transition to next chapter: what if you care about recall and understanding


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{comment}

\chapter{Information Retrieval Structure}
\label{ch:ir_structure}

Topic models, such as Latent Semantic Analysis (LSA) by \citep{deerwester-90} and probabilistic Latent Semantic Indexing~\cite{hofmann-99a}, can capture the semantic relations between documents and queries~\citep{wei-07}.

Topic models have been applied into the information retrieval framework to improve ranking results. Probabilistic language modeling~\citep{croft-03} is a common formalism that incorporates information from topic models.

In this chapter, we briefly introduce the information retrieval framework and the semantic relations between queries and documents. We also present the latent semantic analysis and probabilistic latent semantic indexing models, and how they are combined into information retrieval framework through language modeling.

\section{Semantic Relations in Information Retrieval}

Information Retrieval (IR) systems aim to retrieve relevant documents by comparing query and document texts.

From computer's view, Documents are usually treated as ``bags of words'', and documents are retrieved and ranked by measuring the word overlap. This is consistent with topic models.

Topic models use those bags of words to build abstractions (topics), however.

However, humans would use background knowledge to interpret and understand the queries and ``add'' missing words~\citep{wei-07}, akin to query expansion.

To more accurately retrieve related documents, the semantic relations between queries and documents are needed to improve the ranking results. Topic models, which describe each topic using weighted words and model each document as a distribution over all topics, is an effective way to capture such semantic relations~\citep{deerwester-90,hofmann-99a}.

\section{Topic Models in IR}

Latent Semantic Analysis (LSA) proposed by \citet{deerwester-90} makes use of dimensionality reduction to capture the semantic relations among words and documents.  (Briefly review, note distinction with probabilistic approaches.)

It has been successfully applied into the information retrieval systems through automatic indexing with LSA (Latent Semantic Indexing, LSI) \citep{deerwester-90,dumais-95}.

The probabilistic Latent Semantic Indexing (pLSI), introduced by \citet{hofmann-99a}, is an aspect model which associates an unobserved class (topic) variable with each observed word occurrence. pLSI provides a better fit to text data than LSI, thus quickly gained acceptance in IR systems.

\section{Applying Topic Models into IR}

The language modeling approach~\citep{croft-03,PonteCroft,song-99} is the main framework for using topic models in IR systems, since it has been shown to be effective probabilistic framework for studying information retrieval problem~\citep{PonteCroft,berger-99}.

The basic language modeling framework is to compute the model likelihood of documents for generating the queries. Topic models, which represent documents with topics, offer a new and interesting means to model documents~\citep{wei-07}. A probability mixture model and a term model with back-off smoothing are presented to integrate topic models in this section.

A probability mixture model combines multiple different probability distributions to integrate different factors in IR for query representation or document representation~\citep{miller-99,zhai-01,liu-04}.

A term model with back-off smoothing~\citep{katz-87} is another popular framework to use topic models in IR systems, where a term model is learned for each term in a document and the back-off smoothing~\citep{katz-87} is applied.

% Perhaps it would be nice to mention e-discovery?

Transition to next chapter: what if you care about recall and understanding

\end{comment}